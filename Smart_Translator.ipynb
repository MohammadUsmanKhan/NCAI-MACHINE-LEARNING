{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "Smart Translator.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammadUsmanKhan/NCAI-MACHINE-LEARNING/blob/master/Smart_Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDqBXQaa5_ZA",
        "colab_type": "text"
      },
      "source": [
        "#**GROUP MEMBERS**\n",
        "\n",
        "#**1. Muhammad Imran**\n",
        "\n",
        "#**2. Muhammad Usman Khan**\n",
        "\n",
        "#**3. Zohaib Durrani**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz0D4BgzKirC",
        "colab_type": "text"
      },
      "source": [
        "#**Smart Language Translation**\n",
        "**Using Neural Machine Tranlation**\n",
        "\n",
        "**Recurrent Neural Network (RNN)**\n",
        "\n",
        "**Sequence to sequence Model**\n",
        "\n",
        "**Encoder and Decoder**\n",
        "\n",
        "following below link of dataset and source code https://drive.google.com/drive/folders/1JWiBLIIfnSCpIiSj5TNTZHpp62TmjdKL?usp=sharing\n",
        "\n",
        "*requirement need *\n",
        "\n",
        "download dataset and source code then\n",
        "\n",
        "Using Google Colab editor\n",
        "\n",
        "upload dataset in source code file extension ipynb\n",
        "\n",
        "go to Editor in Notebook Setting Select GPU mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxrYE3qM20Z2",
        "colab_type": "text"
      },
      "source": [
        "Implement an encoder-decoder model with attention which you can read about in the TensorFlow Neural Machine Translation (seq2seq) tutorial. This example uses a more recent set of APIs. This notebook implements the [attention equations] The below picture and formulas are an example of attention mechanism from Luong's paper.\n",
        "\n",
        "attention mechanism\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFj3v9VM5JtO",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1gHPF5vTSD4xAXR2Hi0mLHShnSrdTpcA7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TsMW8TS5j7G",
        "colab_type": "text"
      },
      "source": [
        "The input is put through an encoder model which gives us the encoder output of shape (batch_size, max_length, hidden_size) and the encoder hidden state of shape (batch_size, hidden_size).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx0W8xFkT6bL",
        "colab_type": "text"
      },
      "source": [
        "##**LOADING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6nNjkl8TWzc",
        "colab_type": "code",
        "outputId": "7c1b8b02-6ba1-43bc-ed80-7e3f49cd5740",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ee7276e5-6ce9-4f73-b6af-5dae844e02c9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ee7276e5-6ce9-4f73-b6af-5dae844e02c9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ukr.txt to ukr.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqxLcV0mUolP",
        "colab_type": "code",
        "outputId": "6567754d-d14c-458b-8e95-148333696154",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 57
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-05aebbe9-82d4-4b8b-a168-e7fdb15dc0d6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-05aebbe9-82d4-4b8b-a168-e7fdb15dc0d6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving deu.txt to deu.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGUcwMwFODcn",
        "colab_type": "text"
      },
      "source": [
        "# **English to French Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oRzlfvfJoGO",
        "colab_type": "code",
        "outputId": "9b0d9610-544f-4050-cf7e-af586f1f93ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('fra.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "8000/8000 [==============================] - 24s 3ms/step - loss: 0.9309 - val_loss: 0.9762\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.7393 - val_loss: 0.8061\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.6250 - val_loss: 0.7194\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.5699 - val_loss: 0.6690\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.5274 - val_loss: 0.6330\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4950 - val_loss: 0.6041\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.4690 - val_loss: 0.5869\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.4464 - val_loss: 0.5674\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.4265 - val_loss: 0.5486\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.4092 - val_loss: 0.5443\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.3936 - val_loss: 0.5282\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.3791 - val_loss: 0.5206\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3658 - val_loss: 0.5161\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.3533 - val_loss: 0.5142\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3415 - val_loss: 0.5040\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.3299 - val_loss: 0.4995\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.3196 - val_loss: 0.4943\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.3093 - val_loss: 0.4938\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2999 - val_loss: 0.4883\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2903 - val_loss: 0.4896\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2820 - val_loss: 0.4908\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2735 - val_loss: 0.4882\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2651 - val_loss: 0.4864\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2577 - val_loss: 0.4905\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2501 - val_loss: 0.4913\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2428 - val_loss: 0.4898\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2359 - val_loss: 0.4974\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2295 - val_loss: 0.4968\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2235 - val_loss: 0.4975\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2174 - val_loss: 0.5064\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2112 - val_loss: 0.5026\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2059 - val_loss: 0.5067\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.2006 - val_loss: 0.5049\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1951 - val_loss: 0.5186\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1904 - val_loss: 0.5133\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1859 - val_loss: 0.5193\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1811 - val_loss: 0.5221\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1764 - val_loss: 0.5280\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1722 - val_loss: 0.5344\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1684 - val_loss: 0.5349\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1640 - val_loss: 0.5437\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1605 - val_loss: 0.5441\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1570 - val_loss: 0.5500\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1534 - val_loss: 0.5475\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1502 - val_loss: 0.5545\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1467 - val_loss: 0.5630\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1439 - val_loss: 0.5621\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1405 - val_loss: 0.5640\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1380 - val_loss: 0.5723\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1353 - val_loss: 0.5808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7siInVIJoHr",
        "colab_type": "code",
        "outputId": "f597ad8b-2b32-43f2-95a3-17769ed1140a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: ﻿Go.\n",
            "Decoded sentence: Va !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Sauvez-vous !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Sauvez-vous !\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Faites votre poule.\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Oubliez-le.\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Dépêchez-vous d'aller chez toi !\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Dépêce,isi.\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Cessez de premire !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Cessez de premire !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Cessez de premire !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attends !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attends !\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yT9zblFJoHx",
        "colab_type": "code",
        "outputId": "4ceb9700-4a66-41ff-8edd-6aa8a0ec9f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fra_sent"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\tVa !\\n',\n",
              " '\\tCours\\u202f!\\n',\n",
              " '\\tCourez\\u202f!\\n',\n",
              " '\\tÇa alors\\u202f!\\n',\n",
              " '\\tAu feu !\\n',\n",
              " \"\\tÀ l'aide\\u202f!\\n\",\n",
              " '\\tSaute.\\n',\n",
              " '\\tÇa suffit\\u202f!\\n',\n",
              " '\\tStop\\u202f!\\n',\n",
              " '\\tArrête-toi !\\n',\n",
              " '\\tAttends !\\n',\n",
              " '\\tAttendez !\\n',\n",
              " '\\tJe comprends.\\n',\n",
              " \"\\tJ'essaye.\\n\",\n",
              " \"\\tJ'ai gagné !\\n\",\n",
              " \"\\tJe l'ai emporté !\\n\",\n",
              " '\\tOh non !\\n',\n",
              " '\\tAttaque !\\n',\n",
              " '\\tAttaquez !\\n',\n",
              " '\\tSanté !\\n',\n",
              " '\\tÀ votre santé !\\n',\n",
              " '\\tMerci !\\n',\n",
              " '\\tTchin-tchin !\\n',\n",
              " '\\tLève-toi.\\n',\n",
              " \"\\tJ'ai pigé !\\n\",\n",
              " '\\tCompris !\\n',\n",
              " '\\tPigé\\u202f?\\n',\n",
              " '\\tCompris\\u202f?\\n',\n",
              " \"\\tT'as capté\\u202f?\\n\",\n",
              " '\\tMonte.\\n',\n",
              " '\\tMontez.\\n',\n",
              " '\\tSerre-moi dans tes bras !\\n',\n",
              " '\\tSerrez-moi dans vos bras !\\n',\n",
              " '\\tJe suis tombée.\\n',\n",
              " '\\tJe suis tombé.\\n',\n",
              " '\\tJe sais.\\n',\n",
              " '\\tJe suis parti.\\n',\n",
              " '\\tJe suis partie.\\n',\n",
              " \"\\tJ'ai perdu.\\n\",\n",
              " \"\\tJ'ai 19 ans.\\n\",\n",
              " '\\tJe vais bien.\\n',\n",
              " '\\tÇa va.\\n',\n",
              " '\\tÉcoutez !\\n',\n",
              " \"\\tC'est pas possible\\u202f!\\n\",\n",
              " '\\tImpossible\\u202f!\\n',\n",
              " '\\tEn aucun cas.\\n',\n",
              " \"\\tC'est hors de question !\\n\",\n",
              " \"\\tIl n'en est pas question !\\n\",\n",
              " \"\\tC'est exclu !\\n\",\n",
              " '\\tEn aucune manière !\\n',\n",
              " '\\tHors de question !\\n',\n",
              " '\\tVraiment\\u202f?\\n',\n",
              " '\\tVrai ?\\n',\n",
              " '\\tAh bon ?\\n',\n",
              " '\\tMerci !\\n',\n",
              " '\\tOn essaye.\\n',\n",
              " '\\tNous avons gagné.\\n',\n",
              " '\\tNous gagnâmes.\\n',\n",
              " \"\\tNous l'avons emporté.\\n\",\n",
              " \"\\tNous l'emportâmes.\\n\",\n",
              " '\\tDemande à Tom.\\n',\n",
              " '\\tFantastique\\u202f!\\n',\n",
              " '\\tSois calme !\\n',\n",
              " '\\tSoyez calme !\\n',\n",
              " '\\tSoyez calmes !\\n',\n",
              " '\\tSois détendu !\\n',\n",
              " '\\tSois juste !\\n',\n",
              " '\\tSoyez juste !\\n',\n",
              " '\\tSoyez justes !\\n',\n",
              " '\\tSois équitable !\\n',\n",
              " '\\tSoyez équitable !\\n',\n",
              " '\\tSoyez équitables !\\n',\n",
              " '\\tSois gentil.\\n',\n",
              " '\\tSois gentil !\\n',\n",
              " '\\tSois gentille !\\n',\n",
              " '\\tSoyez gentil !\\n',\n",
              " '\\tSoyez gentille !\\n',\n",
              " '\\tSoyez gentils !\\n',\n",
              " '\\tSoyez gentilles !\\n',\n",
              " '\\tDégage\\u202f!\\n',\n",
              " '\\tAppelle-moi !\\n',\n",
              " '\\tAppellez-moi !\\n',\n",
              " '\\tAppelle-nous !\\n',\n",
              " '\\tAppelez-nous !\\n',\n",
              " '\\tEntrez\\u202f!\\n',\n",
              " '\\tEntre.\\n',\n",
              " '\\tEntre !\\n',\n",
              " '\\tEntrez !\\n',\n",
              " '\\tAllez\\u202f!\\n',\n",
              " '\\tAllez !\\n',\n",
              " '\\tViens !\\n',\n",
              " '\\tVenez !\\n',\n",
              " '\\tLaisse tomber !\\n',\n",
              " '\\tLaissez tomber !\\n',\n",
              " '\\tLaisse-le tomber !\\n',\n",
              " '\\tLaissez-le tomber !\\n',\n",
              " '\\tSortez\\u202f!\\n',\n",
              " '\\tSors !\\n',\n",
              " '\\tSortez !\\n',\n",
              " '\\tSors.\\n',\n",
              " '\\tCasse-toi.\\n',\n",
              " '\\tDégage\\u202f!\\n',\n",
              " '\\tPars !\\n',\n",
              " '\\tVa te faire foutre !\\n',\n",
              " '\\tPars !\\n',\n",
              " '\\tDégage !\\n',\n",
              " '\\tFous le camp !\\n',\n",
              " \"\\tPars d'ici.\\n\",\n",
              " \"\\tVa t'en !\\n\",\n",
              " '\\tDisparais !\\n',\n",
              " '\\tAllez-vous en !\\n',\n",
              " '\\tVa doucement !\\n',\n",
              " '\\tAllez doucement !\\n',\n",
              " '\\tÀ la revoyure.\\n',\n",
              " '\\tAttends un peu !\\n',\n",
              " '\\tAttendez un peu !\\n',\n",
              " '\\tTiens bon !\\n',\n",
              " '\\tTenez bon !\\n',\n",
              " '\\tIl laissa tomber.\\n',\n",
              " '\\tIl a laissé tomber.\\n',\n",
              " '\\tIl court.\\n',\n",
              " '\\tAide-moi !\\n',\n",
              " '\\tAide-moi.\\n',\n",
              " '\\tAidez-moi.\\n',\n",
              " '\\tAidez-nous !\\n',\n",
              " '\\tAide-nous !\\n',\n",
              " '\\tNe bouge plus !\\n',\n",
              " '\\tNe quittez pas.\\n',\n",
              " '\\tJe suis du même avis.\\n',\n",
              " '\\tJe conduis.\\n',\n",
              " \"\\tJ'essayai.\\n\",\n",
              " \"\\tJ'ai essayé.\\n\",\n",
              " \"\\tJ'ai tenté.\\n\",\n",
              " \"\\tJ'irai.\\n\",\n",
              " '\\tJe suis gras.\\n',\n",
              " '\\tJe suis gros.\\n',\n",
              " '\\tJe suis en forme.\\n',\n",
              " '\\tJe suis touché !\\n',\n",
              " '\\tJe suis touchée !\\n',\n",
              " '\\tJe suis malade.\\n',\n",
              " '\\tJe suis triste.\\n',\n",
              " '\\tJe suis timide.\\n',\n",
              " '\\tJe suis mouillé.\\n',\n",
              " '\\tJe suis mouillée.\\n',\n",
              " \"\\tC'est bibi\\u202f!\\n\",\n",
              " '\\tJoignez-vous.\\n',\n",
              " '\\tJoignez-vous à nous.\\n',\n",
              " '\\tGarde-le !\\n',\n",
              " '\\tGardez-le !\\n',\n",
              " '\\tEmbrasse-moi.\\n',\n",
              " '\\tEmbrassez-moi.\\n',\n",
              " '\\tMoi aussi.\\n',\n",
              " '\\tOuvre-moi\\u202f!\\n',\n",
              " '\\tOuvre.\\n',\n",
              " '\\tParfait\\u202f!\\n',\n",
              " '\\tÀ plus.\\n',\n",
              " '\\tMontre-moi !\\n',\n",
              " '\\tMontrez-moi !\\n',\n",
              " '\\tTaisez-vous\\u202f!\\n',\n",
              " '\\tFerme-la\\u202f!\\n',\n",
              " '\\tTais-toi !\\n',\n",
              " '\\tFerme-la !\\n',\n",
              " '\\tLa ferme !\\n',\n",
              " '\\tÀ plus tard !\\n',\n",
              " '\\tPrends-le !\\n',\n",
              " '\\tPrenez-le !\\n',\n",
              " '\\tDis-moi !\\n',\n",
              " '\\tDites-moi !\\n',\n",
              " '\\tTom a gagné.\\n',\n",
              " '\\tRéveille-toi\\u202f!\\n',\n",
              " '\\tRéveille-toi !\\n',\n",
              " '\\tRéveillez-vous !\\n',\n",
              " '\\tRéveille-toi !\\n',\n",
              " '\\tRéveillez-vous !\\n',\n",
              " '\\tLave-toi !\\n',\n",
              " '\\tLavez-vous !\\n',\n",
              " '\\tNous savons.\\n',\n",
              " '\\tNous perdîmes.\\n',\n",
              " '\\tNous avons perdu.\\n',\n",
              " '\\tNous fûmes battus.\\n',\n",
              " '\\tNous fûmes battues.\\n',\n",
              " '\\tNous fûmes défaits.\\n',\n",
              " '\\tNous fûmes défaites.\\n',\n",
              " '\\tNous avons été défaits.\\n',\n",
              " '\\tNous avons été défaites.\\n',\n",
              " '\\tNous avons été battus.\\n',\n",
              " '\\tNous avons été battues.\\n',\n",
              " '\\tQui a gagné ?\\n',\n",
              " \"\\tQui l'a emporté ?\\n\",\n",
              " '\\tTu cours.\\n',\n",
              " '\\tSuis-je gros ?\\n',\n",
              " '\\tSuis-je grosse ?\\n',\n",
              " '\\tRecule\\u2009!\\n',\n",
              " '\\tReculez.\\n',\n",
              " '\\tRetire-toi\\u2009!\\n',\n",
              " '\\tRetirez-vous.\\n',\n",
              " '\\tSois un homme !\\n',\n",
              " '\\tSoyez un homme !\\n',\n",
              " '\\tSois calme !\\n',\n",
              " '\\tSoyez calme !\\n',\n",
              " '\\tSoyez calmes !\\n',\n",
              " '\\tAucune idée.\\n',\n",
              " \"\\tJ'en sais foutre rien.\\n\",\n",
              " '\\tAppelle Tom.\\n',\n",
              " '\\tAppelez Tom.\\n',\n",
              " '\\tCourage\\u202f!\\n',\n",
              " '\\tDétends-toi\\u202f!\\n',\n",
              " '\\tMenottez-le.\\n',\n",
              " '\\tAvance !\\n',\n",
              " '\\tAvancez !\\n',\n",
              " '\\tContinue à rouler !\\n',\n",
              " '\\tContinuez à rouler !\\n',\n",
              " '\\tLâche-toi !\\n',\n",
              " '\\tDescends !\\n',\n",
              " '\\tDescendez !\\n',\n",
              " '\\tLâche-toi !\\n',\n",
              " '\\tLâchez-vous !\\n',\n",
              " \"\\tVa voir ailleurs si j'y suis\\u202f!\\n\",\n",
              " '\\tDégage\\u202f!\\n',\n",
              " '\\tVa au diable !\\n',\n",
              " '\\tSois réaliste !\\n',\n",
              " '\\tVas-y.\\n',\n",
              " '\\tPoursuis !\\n',\n",
              " '\\tPasse devant !\\n',\n",
              " '\\tVas-y !\\n',\n",
              " '\\tBien joué\\u202f!\\n',\n",
              " '\\tBon boulot\\u202f!\\n',\n",
              " '\\tBeau travail\\u202f!\\n',\n",
              " '\\tAttrape-le.\\n',\n",
              " '\\tAttrapez-le.\\n',\n",
              " '\\tAmuse-toi bien !\\n',\n",
              " '\\tAmusez-vous bien !\\n',\n",
              " '\\tIl essaye.\\n',\n",
              " '\\tIl est mouillé.\\n',\n",
              " '\\tSalut, les mecs !\\n',\n",
              " \"\\tComme c'est mignon\\u202f!\\n\",\n",
              " '\\tQuelle profondeur\\u202f?\\n',\n",
              " \"\\tComme c'est chouette !\\n\",\n",
              " \"\\tComme c'est gentil !\\n\",\n",
              " \"\\tC'est du joli !\\n\",\n",
              " \"\\tComme c'est agréable !\\n\",\n",
              " '\\tFais-moi rire.\\n',\n",
              " '\\tDépêche-toi.\\n',\n",
              " '\\tGrouille\\u202f!\\n',\n",
              " '\\tPressez-vous !\\n',\n",
              " '\\tFiça !\\n',\n",
              " '\\tJe suis gras.\\n',\n",
              " \"\\tJe m'en suis bien sorti.\\n\",\n",
              " \"\\tJe m'en suis bien sortie.\\n\",\n",
              " \"\\tJe l'ai fait.\\n\",\n",
              " \"\\tC'est moi qui l'ai fait.\\n\",\n",
              " \"\\tJ'ai échoué.\\n\",\n",
              " \"\\tJ'ai oublié.\\n\",\n",
              " \"\\tJ'ai compris.\\n\",\n",
              " \"\\tJ'ai compris.\\n\",\n",
              " \"\\tJ'ai capté.\\n\",\n",
              " '\\tJe téléphonai.\\n',\n",
              " \"\\tJ'ai téléphoné.\\n\",\n",
              " '\\tJe refuse.\\n',\n",
              " '\\tJe le refuse.\\n',\n",
              " \"\\tJe l'ai vu.\\n\",\n",
              " '\\tJe l’ai vu.\\n',\n",
              " '\\tJe suis resté.\\n',\n",
              " '\\tJe suis restée.\\n',\n",
              " \"\\tJe l'utilise.\\n\",\n",
              " \"\\tJ'en fais usage.\\n\",\n",
              " \"\\tJe m'en sers.\\n\",\n",
              " '\\tJe paierai.\\n',\n",
              " '\\tJe vais essayer.\\n',\n",
              " \"\\tJ'essaierai.\\n\",\n",
              " '\\tJe suis revenu.\\n',\n",
              " '\\tMe revoilà.\\n',\n",
              " '\\tJe suis chauve.\\n',\n",
              " '\\tJe suis occupé.\\n',\n",
              " '\\tJe suis occupée.\\n',\n",
              " '\\tJe suis calme.\\n',\n",
              " \"\\tJ'ai froid.\\n\",\n",
              " '\\tJe suis sourd.\\n',\n",
              " '\\tJe suis sourde.\\n',\n",
              " \"\\tJ'en ai fini.\\n\",\n",
              " '\\tJe suis rapide.\\n',\n",
              " '\\tTout va bien.\\n',\n",
              " '\\tJe vais bien.\\n',\n",
              " '\\tÇa va.\\n',\n",
              " '\\tJe suis libre !\\n',\n",
              " '\\tJe suis libre.\\n',\n",
              " '\\tJe suis disponible.\\n',\n",
              " '\\tJe suis repu\\u202f!\\n',\n",
              " '\\tJe suis rassasié\\u202f!\\n',\n",
              " '\\tJe suis content.\\n',\n",
              " '\\tJe suis chez moi.\\n',\n",
              " '\\tJe suis en retard.\\n',\n",
              " '\\tJe suis paresseux.\\n',\n",
              " '\\tJe suis fainéant.\\n',\n",
              " '\\tJe suis paresseuse.\\n',\n",
              " '\\tJe suis fainéante.\\n',\n",
              " '\\tJe vais bien.\\n',\n",
              " '\\tJe me porte bien.\\n',\n",
              " '\\tJe suis en sécurité.\\n',\n",
              " '\\tJe suis malade.\\n',\n",
              " \"\\tJ'en suis certain.\\n\",\n",
              " '\\tJe suis certain.\\n',\n",
              " \"\\tJ'en suis sûr.\\n\",\n",
              " \"\\tJ'en suis sûre.\\n\",\n",
              " '\\tJe suis grande.\\n',\n",
              " '\\tJe suis mince.\\n',\n",
              " '\\tJe suis ordonné.\\n',\n",
              " '\\tJe suis ordonnée.\\n',\n",
              " '\\tJe suis laid.\\n',\n",
              " '\\tJe suis laide.\\n',\n",
              " '\\tJe suis faible.\\n',\n",
              " '\\tJe vais bien.\\n',\n",
              " '\\tJe me porte bien.\\n',\n",
              " \"\\tJ'ai gagné.\\n\",\n",
              " \"\\tJe l'ai emporté.\\n\",\n",
              " '\\tElle marche.\\n',\n",
              " '\\tÇa fonctionne.\\n',\n",
              " \"\\tC'est le sien.\\n\",\n",
              " \"\\tC'est la sienne.\\n\",\n",
              " \"\\tC'est nouveau.\\n\",\n",
              " \"\\tC'est neuf.\\n\",\n",
              " \"\\tC'est bizarre.\\n\",\n",
              " '\\tC’est triste.\\n',\n",
              " \"\\tDéfense d'entrer.\\n\",\n",
              " \"\\tN'entrez pas.\\n\",\n",
              " '\\tLaisse tomber !\\n',\n",
              " '\\tLaissez tomber !\\n',\n",
              " '\\tLaissez-moi !\\n',\n",
              " '\\tLaisse-nous !\\n',\n",
              " '\\tLaissez-nous !\\n',\n",
              " '\\tAllons-y !\\n',\n",
              " '\\tAllons !\\n',\n",
              " '\\tAllons-y !\\n',\n",
              " '\\tAttention !\\n',\n",
              " '\\tÉpouse-moi !\\n',\n",
              " '\\tÉpousez-moi !\\n',\n",
              " '\\tPuis-je partir ?\\n',\n",
              " '\\tPuis-je y aller ?\\n',\n",
              " \"\\tPuis-je m'y rendre ?\\n\",\n",
              " '\\tElle est venue.\\n',\n",
              " '\\tElle est morte.\\n',\n",
              " '\\tElle court.\\n',\n",
              " '\\tAssieds-toi !\\n',\n",
              " '\\tAsseyez-vous !\\n',\n",
              " '\\tAssieds-toi ici.\\n',\n",
              " '\\tAsseyez-vous ici.\\n',\n",
              " '\\tParle plus fort\\u202f!\\n',\n",
              " '\\tParlez plus fort\\u202f!\\n',\n",
              " '\\tArrête Tom.\\n',\n",
              " '\\tStoppez Tom.\\n',\n",
              " '\\tGénial\\u202f!\\n',\n",
              " '\\tExcellent\\u202f!\\n',\n",
              " '\\tFormidable !\\n',\n",
              " '\\tIls gagnèrent.\\n',\n",
              " '\\tElles gagnèrent.\\n',\n",
              " '\\tIls ont gagné.\\n',\n",
              " '\\tElles ont gagné.\\n',\n",
              " '\\tTom est venu.\\n',\n",
              " '\\tTom est mort.\\n',\n",
              " '\\tTom est parti.\\n',\n",
              " '\\tTom partit.\\n',\n",
              " '\\tTom a perdu.\\n',\n",
              " '\\tTrop tard.\\n',\n",
              " '\\tFaites-moi confiance.\\n',\n",
              " '\\tFais-moi confiance.\\n',\n",
              " '\\tFais un effort.\\n',\n",
              " '\\tEssaies-en !\\n',\n",
              " '\\tEssayez-en !\\n',\n",
              " '\\tEssaie ceci !\\n',\n",
              " '\\tEssayez ceci !\\n',\n",
              " '\\tUtilise ceci.\\n',\n",
              " '\\tUtilisez ceci.\\n',\n",
              " '\\tEmploie ceci !\\n',\n",
              " '\\tEmployez ceci !\\n',\n",
              " '\\tAvertis Tom.\\n',\n",
              " '\\tPréviens Tom.\\n',\n",
              " '\\tRegarde-moi !\\n',\n",
              " '\\tRegardez-moi !\\n',\n",
              " '\\tRegardez-nous !\\n',\n",
              " '\\tRegarde-nous !\\n',\n",
              " \"\\tNous sommes d'accord.\\n\",\n",
              " '\\tNous irons.\\n',\n",
              " '\\tPour quoi faire\\u202f?\\n',\n",
              " '\\tÀ quoi bon ?\\n',\n",
              " \"\\tQu'est-ce qu'on s'est marrés !\\n\",\n",
              " \"\\tQu'est-ce qu'on s'est marrées !\\n\",\n",
              " '\\tQui est mort ?\\n',\n",
              " '\\tQui est-il\\u202f?\\n',\n",
              " '\\tÉcris-moi !\\n',\n",
              " '\\tÉcrivez-moi !\\n',\n",
              " '\\tAprès vous.\\n',\n",
              " '\\tEn joue ! Feu !\\n',\n",
              " '\\tSuis-je en retard ?\\n',\n",
              " '\\tRépondez-moi.\\n',\n",
              " '\\tAssieds-toi !\\n',\n",
              " '\\tAsseyez-vous !\\n',\n",
              " '\\tLes oiseaux volent.\\n',\n",
              " '\\tÀ tes souhaits\\u202f!\\n',\n",
              " '\\tAppelle à la maison !\\n',\n",
              " '\\tCalmez-vous !\\n',\n",
              " '\\tCalme-toi.\\n',\n",
              " '\\tPouvons-nous partir ?\\n',\n",
              " '\\tPouvons-nous nous en aller ?\\n',\n",
              " '\\tPouvons-nous y aller ?\\n',\n",
              " '\\tRattrape-le.\\n',\n",
              " '\\tReviens !\\n',\n",
              " '\\tRevenez !\\n',\n",
              " '\\tViens ici.\\n',\n",
              " '\\tVenez là.\\n',\n",
              " '\\tViens\\u202f!\\n',\n",
              " '\\tVenez\\u202f!\\n',\n",
              " '\\tVenez ici !\\n',\n",
              " '\\tViens chez nous !\\n',\n",
              " '\\tVenez chez nous !\\n',\n",
              " '\\tViens chez moi !\\n',\n",
              " '\\tVenez chez moi !\\n',\n",
              " '\\tViens bientôt !\\n',\n",
              " '\\tVenez bientôt !\\n',\n",
              " '\\tCalmez-vous !\\n',\n",
              " '\\tAi-je gagné ?\\n',\n",
              " \"\\tL'ai-je emporté ?\\n\",\n",
              " '\\tEst-ce moi qui ai gagné ?\\n',\n",
              " '\\tDes chiens aboient.\\n',\n",
              " '\\tLes chiens aboient.\\n',\n",
              " '\\tNe demande pas !\\n',\n",
              " '\\tNe pleure pas !\\n',\n",
              " '\\tNe meurs pas !\\n',\n",
              " '\\tNe mourez pas !\\n',\n",
              " '\\tNe mens pas.\\n',\n",
              " '\\tNe courez pas.\\n',\n",
              " '\\tNe cours pas.\\n',\n",
              " '\\tExcuse-moi.\\n',\n",
              " '\\tFantastique\\u202f!\\n',\n",
              " '\\tSens ça !\\n',\n",
              " '\\tSentez ça !\\n',\n",
              " '\\tTouche ça !\\n',\n",
              " '\\tTouchez ça !\\n',\n",
              " '\\tSuis-moi.\\n',\n",
              " '\\tSuis-nous !\\n',\n",
              " '\\tSuivez-nous !\\n',\n",
              " '\\tOublie !\\n',\n",
              " '\\tOublie-le !\\n',\n",
              " '\\tOubliez !\\n',\n",
              " '\\tOubliez-le !\\n',\n",
              " '\\tLaisse tomber.\\n',\n",
              " '\\tOublie.\\n',\n",
              " '\\tTrouve un emploi !\\n',\n",
              " '\\tTrouve un boulot !\\n',\n",
              " '\\tTrouvez un emploi !\\n',\n",
              " '\\tTrouvez un boulot !\\n',\n",
              " '\\tPrépare-toi.\\n',\n",
              " '\\tPréparez-vous.\\n',\n",
              " '\\tVa le chercher !\\n',\n",
              " '\\tAllez le chercher !\\n',\n",
              " '\\tEntrez\\u202f!\\n',\n",
              " '\\tVa au lit !\\n',\n",
              " '\\tAllez au lit !\\n',\n",
              " '\\tBonne chance\\u202f!\\n',\n",
              " '\\tBonne chance.\\n',\n",
              " '\\tAttrape ça !\\n',\n",
              " '\\tAttrapez ça !\\n',\n",
              " '\\tSaisis-toi de ça !\\n',\n",
              " '\\tSaisissez-vous de ça !\\n',\n",
              " '\\tAttrape ça !\\n',\n",
              " '\\tAttrapez ça !\\n',\n",
              " '\\tPas touche\\u202f!\\n',\n",
              " '\\tIl est malade.\\n',\n",
              " '\\tIl est vieux.\\n',\n",
              " '\\tIl est bon.\\n',\n",
              " '\\tIl est paresseux.\\n',\n",
              " '\\tIl est riche.\\n',\n",
              " '\\tMe voici.\\n',\n",
              " '\\tVoilà cinq dollars.\\n',\n",
              " '\\tHalte au feu !\\n',\n",
              " '\\tCessez le feu !\\n',\n",
              " '\\tTiens ça !\\n',\n",
              " '\\tTenez ça !\\n',\n",
              " '\\tTenez ceci !\\n',\n",
              " '\\tTiens ceci !\\n',\n",
              " \"\\tC'est affreux\\u202f!\\n\",\n",
              " '\\tComment Tom va-t-il ?\\n',\n",
              " '\\tComment va Tom ?\\n',\n",
              " '\\tJe suis occupé.\\n',\n",
              " '\\tJe suis calme.\\n',\n",
              " \"\\tJ'ai froid.\\n\",\n",
              " '\\tJe vais bien.\\n',\n",
              " '\\tJe suis bon.\\n',\n",
              " '\\tJe suis ici.\\n',\n",
              " '\\tJe suis paresseux.\\n',\n",
              " '\\tJe suis fainéant.\\n',\n",
              " '\\tJe suis paresseuse.\\n',\n",
              " '\\tJe suis fainéante.\\n',\n",
              " '\\tJe vais bien.\\n',\n",
              " '\\tJe suis malade.\\n',\n",
              " '\\tJe suis sûr.\\n',\n",
              " '\\tJe suis certain.\\n',\n",
              " '\\tJe suis faible.\\n',\n",
              " '\\tJe vous en prie.\\n',\n",
              " '\\tJe vous en conjure.\\n',\n",
              " '\\tJe vous en supplie.\\n',\n",
              " '\\tJe te prie.\\n',\n",
              " '\\tJe sais courir.\\n',\n",
              " '\\tJe sais skier.\\n',\n",
              " \"\\tJ'eus un mouvement de recul.\\n\",\n",
              " \"\\tJ'ai eu un mouvement de recul.\\n\",\n",
              " '\\tJe suis rentré en moi-même.\\n',\n",
              " \"\\tJ'abandonne.\\n\",\n",
              " '\\tJe me suis mis à avoir chaud.\\n',\n",
              " '\\tJe me suis mise à avoir chaud.\\n',\n",
              " '\\tJe me suis amusé.\\n',\n",
              " '\\tJe me suis amusée.\\n',\n",
              " '\\tJe me suis marré.\\n',\n",
              " '\\tJe me suis marrée.\\n',\n",
              " '\\tJe déteste ça.\\n',\n",
              " \"\\tJ'espère bien.\\n\",\n",
              " '\\tJe le savais.\\n',\n",
              " \"\\tJ'aime ça.\\n\",\n",
              " '\\tJe l’ai perdu.\\n',\n",
              " \"\\tJ'adore ça !\\n\",\n",
              " \"\\tJ'adore ça !\\n\",\n",
              " '\\tJe suis sérieux\\u202f!\\n',\n",
              " '\\tJe suis sérieux.\\n',\n",
              " '\\tJe dois y aller.\\n',\n",
              " \"\\tIl faut que j'y aille.\\n\",\n",
              " '\\tIl me faut y aller.\\n',\n",
              " '\\tIl me faut partir.\\n',\n",
              " \"\\tIl me faut m'en aller.\\n\",\n",
              " '\\tJe dois partir.\\n',\n",
              " \"\\tJe dois m'en aller.\\n\",\n",
              " \"\\tIl faut que je m'en aille.\\n\",\n",
              " \"\\tJ'en ai besoin.\\n\",\n",
              " '\\tIl me le faut.\\n',\n",
              " \"\\tJ'ai remarqué.\\n\",\n",
              " '\\tJe le promets.\\n',\n",
              " '\\tJe me suis détendu.\\n',\n",
              " '\\tJe me suis détendue.\\n',\n",
              " \"\\tJ'ai dit non.\\n\",\n",
              " \"\\tJe l'ai dit.\\n\",\n",
              " \"\\tJe l'ai vu.\\n\",\n",
              " '\\tJe l’ai vu.\\n',\n",
              " '\\tJe le vis.\\n',\n",
              " \"\\tJ'en ai vu une.\\n\",\n",
              " \"\\tJ'en ai vu un.\\n\",\n",
              " '\\tJe vous vis.\\n',\n",
              " '\\tJe te vis.\\n',\n",
              " \"\\tJe t'ai vue.\\n\",\n",
              " \"\\tJe t'ai vu.\\n\",\n",
              " '\\tJe vous ai vues.\\n',\n",
              " '\\tJe vous ai vus.\\n',\n",
              " '\\tJe vous ai vue.\\n',\n",
              " '\\tJe vous ai vu.\\n',\n",
              " '\\tJe vois Tom.\\n',\n",
              " \"\\tJ'ai crié.\\n\",\n",
              " \"\\tJ'ai trébuché.\\n\",\n",
              " \"\\tJ'ai plané.\\n\",\n",
              " '\\tJe le veux.\\n',\n",
              " \"\\tJ'étais nouveau.\\n\",\n",
              " \"\\tJ'étais nouvelle.\\n\",\n",
              " \"\\tJ'irai.\\n\",\n",
              " '\\tJe me suis réveillé.\\n',\n",
              " '\\tJe me suis éveillé.\\n',\n",
              " \"\\tJe serais d'accord.\\n\",\n",
              " '\\tJe partirais.\\n',\n",
              " \"\\tJ'appellerai.\\n\",\n",
              " '\\tJe cuisinerai.\\n',\n",
              " \"\\tJ'aiderai.\\n\",\n",
              " '\\tJe vivrai.\\n',\n",
              " \"\\tJ'obéirai.\\n\",\n",
              " '\\tJe ferai mon sac.\\n',\n",
              " '\\tJe ferai ma valise.\\n',\n",
              " '\\tJe plierai mes gaules.\\n',\n",
              " '\\tJe passerai.\\n',\n",
              " \"\\tJ'abandonnerai.\\n\",\n",
              " '\\tJe chanterai.\\n',\n",
              " \"\\tJ'arrêterai.\\n\",\n",
              " '\\tJe nagerai.\\n',\n",
              " \"\\tJ'attendrai.\\n\",\n",
              " '\\tJe marcherai.\\n',\n",
              " '\\tJe vais travailler.\\n',\n",
              " '\\tJe travaillerai.\\n',\n",
              " '\\tJe suis flic.\\n',\n",
              " '\\tJe suis un homme.\\n',\n",
              " '\\tJe suis en vie.\\n',\n",
              " '\\tJe suis vivant.\\n',\n",
              " '\\tJe suis vivante.\\n',\n",
              " '\\tJe suis seule.\\n',\n",
              " '\\tJe suis seul.\\n',\n",
              " '\\tJe suis armé.\\n',\n",
              " '\\tJe suis armée.\\n',\n",
              " '\\tJe suis réveillé.\\n',\n",
              " '\\tJe suis aveugle.\\n',\n",
              " '\\tJe suis fauché.\\n',\n",
              " '\\tJe suis fou.\\n',\n",
              " '\\tJe suis folle.\\n',\n",
              " '\\tJe suis guéri.\\n',\n",
              " '\\tJe suis guérie.\\n',\n",
              " '\\tJe suis saoul.\\n',\n",
              " '\\tJe suis soûl.\\n',\n",
              " '\\tJe suis ivre.\\n',\n",
              " '\\tJe me meurs.\\n',\n",
              " '\\tJe suis en avance.\\n',\n",
              " '\\tJe suis en premier.\\n',\n",
              " '\\tJe suis difficile.\\n',\n",
              " '\\tJe suis tatillon.\\n',\n",
              " '\\tJe suis tatillonne.\\n',\n",
              " '\\tJe pars maintenant.\\n',\n",
              " '\\tJe me tire.\\n',\n",
              " '\\tJ’y vais.\\n',\n",
              " '\\tJe pars.\\n',\n",
              " '\\tJe suis loyal.\\n',\n",
              " '\\tJe suis loyale.\\n',\n",
              " '\\tJe suis veinard.\\n',\n",
              " '\\tJe suis veinarde.\\n',\n",
              " \"\\tJ'ai du pot.\\n\",\n",
              " '\\tJe suis chanceux.\\n',\n",
              " '\\tJe suis chanceuse.\\n',\n",
              " '\\tJe suis en train de mentir.\\n',\n",
              " '\\tJe suis tranquille.\\n',\n",
              " '\\tJe suis prête !\\n',\n",
              " '\\tJe suis prêt !\\n',\n",
              " '\\tJe suis prêt.\\n',\n",
              " \"\\tJ'ai raison.\\n\",\n",
              " '\\tJe suis sobre.\\n',\n",
              " '\\tExcuse-moi.\\n',\n",
              " '\\tDésolé.\\n',\n",
              " '\\tDésolé !\\n',\n",
              " '\\tJe suis désolé.\\n',\n",
              " '\\tJe suis désolée.\\n',\n",
              " '\\tJe suis coincée.\\n',\n",
              " '\\tJe suis timide.\\n',\n",
              " '\\tJe suis fatigué !\\n',\n",
              " '\\tJe suis dur.\\n',\n",
              " '\\tJe suis dure.\\n',\n",
              " '\\tJe suis dur à cuire.\\n',\n",
              " '\\tJe suis dure à cuire.\\n',\n",
              " '\\tJe suis à toi.\\n',\n",
              " '\\tJe suis à vous.\\n',\n",
              " \"\\tJ'ai perdu.\\n\",\n",
              " '\\tEst-ce que Tom va bien ?\\n',\n",
              " '\\tTom va-t-il bien ?\\n',\n",
              " \"\\tC'est grave\\u202f?\\n\",\n",
              " '\\tEst-ce éloigné ?\\n',\n",
              " '\\tEst-ce loin ?\\n',\n",
              " '\\tEst-ce toi ?\\n',\n",
              " '\\tEst-ce vous ?\\n',\n",
              " \"\\tEst-ce que c'est vous ?\\n\",\n",
              " '\\tÇa a échoué.\\n',\n",
              " \"\\tC'est nouveau.\\n\",\n",
              " \"\\tC'est neuf.\\n\",\n",
              " '\\tIl a neigé.\\n',\n",
              " '\\tÇa sent mauvais.\\n',\n",
              " '\\tÇa pue.\\n',\n",
              " '\\tÇa a fonctionné.\\n',\n",
              " '\\tÇa a marché.\\n',\n",
              " '\\tIl est trois heures et demie.\\n',\n",
              " '\\tIl fait froid.\\n',\n",
              " \"\\tC'est froid.\\n\",\n",
              " \"\\tC'est sombre.\\n\",\n",
              " '\\tElle est morte.\\n',\n",
              " \"\\tC'est mort.\\n\",\n",
              " '\\tIl est mort.\\n',\n",
              " \"\\tC'est fait.\\n\",\n",
              " \"\\tC'est de la nourriture.\\n\",\n",
              " \"\\tC'est gratuit.\\n\",\n",
              " \"\\tC'est le sien.\\n\",\n",
              " \"\\tC'est la sienne.\\n\",\n",
              " '\\tIl est tard.\\n',\n",
              " \"\\tC'est perdu.\\n\",\n",
              " \"\\tC'est ouvert.\\n\",\n",
              " \"\\tC'est le nôtre.\\n\",\n",
              " \"\\tC'est la nôtre.\\n\",\n",
              " \"\\tC'est à nous.\\n\",\n",
              " \"\\tC'est du sable.\\n\",\n",
              " \"\\tC'est vrai\\u202f!\\n\",\n",
              " \"\\tC'est du boulot.\\n\",\n",
              " '\\tAinsi soit-il.\\n',\n",
              " '\\tLaisse faire.\\n',\n",
              " '\\tLaisse-moi partir\\u202f!\\n',\n",
              " '\\tLaissez-moi partir\\u202f!\\n',\n",
              " '\\tLâche-moi\\u202f!\\n',\n",
              " \"\\tLaisse-moi m'en aller !\\n\",\n",
              " \"\\tLaissez-moi m'en aller !\\n\",\n",
              " '\\tLaissez-moi y aller !\\n',\n",
              " '\\tLaisse-moi y aller !\\n',\n",
              " '\\tLaisse-moi partir\\u202f!\\n',\n",
              " '\\tLaissez-moi partir\\u202f!\\n',\n",
              " \"\\tLaisse-moi m'en aller !\\n\",\n",
              " \"\\tLaissez-moi m'en aller !\\n\",\n",
              " '\\tLaissez-moi rentrer.\\n',\n",
              " '\\tLaissez-moi entrer.\\n',\n",
              " '\\tDemandons.\\n',\n",
              " '\\tVoyons voir !\\n',\n",
              " '\\tReste allongé, immobile !\\n',\n",
              " '\\tReste allongée, immobile !\\n',\n",
              " '\\tRestez allongé, immobile !\\n',\n",
              " '\\tRestez allongée, immobile !\\n',\n",
              " '\\tRestez allongés, immobiles !\\n',\n",
              " '\\tRestez allongées, immobiles !\\n',\n",
              " '\\tÉchauffe-toi !\\n',\n",
              " '\\tÉchauffez-vous !\\n',\n",
              " '\\tDétends-toi !\\n',\n",
              " '\\tLaisse-toi aller !\\n',\n",
              " '\\tLaissez-vous aller !\\n',\n",
              " '\\tJoli coup !\\n',\n",
              " '\\tPour sûr.\\n',\n",
              " '\\tMais ouais !\\n',\n",
              " '\\tBien sûr.\\n',\n",
              " '\\tPour sûr.\\n',\n",
              " '\\tJe vous en prie !\\n',\n",
              " \"\\tJe t'en prie !\\n\",\n",
              " '\\tPardon\\u202f?\\n',\n",
              " '\\tJe vous demande pardon\\u202f?\\n',\n",
              " '\\tPlaît-il\\u202f?\\n',\n",
              " '\\tLis ceci.\\n',\n",
              " '\\tDis bonjour.\\n',\n",
              " '\\tVoyez ci-dessus.\\n',\n",
              " '\\tVraiment\\u202f?\\n',\n",
              " '\\tEst-ce sérieux\\u202f?\\n',\n",
              " '\\tSérieusement ?\\n',\n",
              " '\\tElle pleurait.\\n',\n",
              " '\\tElle pleura.\\n',\n",
              " '\\tElle a essayé.\\n',\n",
              " '\\tElle marche.\\n',\n",
              " '\\tElle est chaude.\\n',\n",
              " '\\tElle est très attirante.\\n',\n",
              " '\\tSigne ici.\\n',\n",
              " '\\tSignez ici.\\n',\n",
              " '\\tRalentis !\\n',\n",
              " '\\tRalentissez !\\n',\n",
              " '\\tReste en arrière !\\n',\n",
              " '\\tRestez en arrière !\\n',\n",
              " '\\tRestez calme.\\n',\n",
              " '\\tReste calme.\\n',\n",
              " '\\tGarde ton calme.\\n',\n",
              " '\\tGarde ton sang-froid.\\n',\n",
              " '\\tReste tranquille.\\n',\n",
              " '\\tReste baissé.\\n',\n",
              " '\\tRestez baissé.\\n',\n",
              " '\\tReste mince !\\n',\n",
              " '\\tArrêtez !\\n',\n",
              " '\\tArrêtez ça !\\n',\n",
              " '\\tArrête ça !\\n',\n",
              " '\\tPrends soin de toi.\\n',\n",
              " '\\tPrenez soin de vous.\\n',\n",
              " '\\tPrends le mien.\\n',\n",
              " '\\tPrends la mienne.\\n',\n",
              " '\\tPrenez le mien.\\n',\n",
              " '\\tPrenez la mienne.\\n',\n",
              " '\\tPrends les miens.\\n',\n",
              " '\\tPrends les miennes.\\n',\n",
              " '\\tPrenez les miens.\\n',\n",
              " '\\tPrenez les miennes.\\n',\n",
              " '\\tPrends ça.\\n',\n",
              " '\\tPrenez ça.\\n',\n",
              " '\\tMerci !\\n',\n",
              " \"\\tC'est ça.\\n\",\n",
              " '\\tIls sont tombés.\\n',\n",
              " '\\tElles sont tombées.\\n',\n",
              " '\\tIls sont partis.\\n',\n",
              " '\\tElles sont parties.\\n',\n",
              " '\\tIls ont menti.\\n',\n",
              " '\\tElles ont menti.\\n',\n",
              " '\\tIls ont perdu.\\n',\n",
              " '\\tElles ont perdu.\\n',\n",
              " '\\tIls nageaient.\\n',\n",
              " '\\tElles nageaient.\\n',\n",
              " '\\tIls nagèrent.\\n',\n",
              " '\\tElles nagèrent.\\n',\n",
              " '\\tTom tricote.\\n',\n",
              " '\\tTom sait.\\n',\n",
              " '\\tTom a parlé.\\n',\n",
              " '\\tTom est gros.\\n',\n",
              " '\\tEssaie encore.\\n',\n",
              " '\\tEssayez de nouveau.\\n',\n",
              " '\\tEssaie de nouveau.\\n',\n",
              " '\\tEssaie-le\\u2009!\\n',\n",
              " '\\tTourne à gauche.\\n',\n",
              " '\\tAttends ici.\\n',\n",
              " '\\tAttends là.\\n',\n",
              " '\\tAttendez ici.\\n',\n",
              " '\\tAttendez là.\\n',\n",
              " '\\tAttention !\\n',\n",
              " '\\tFaites attention\\u202f!\\n',\n",
              " '\\tFais attention\\u202f!\\n',\n",
              " \"\\tNous sommes tombés d'accord.\\n\",\n",
              " '\\tNous avons réussi\\u202f!\\n',\n",
              " '\\tNous avons réussi\\u202f!\\n',\n",
              " \"\\tNous l'avons fait.\\n\",\n",
              " '\\tNous avons oublié.\\n',\n",
              " \"\\tNous l'avons vu.\\n\",\n",
              " \"\\tNous l'avons vue.\\n\",\n",
              " '\\tNous discutâmes.\\n',\n",
              " '\\tNous avons discuté.\\n',\n",
              " '\\tNous nous sommes entretenus.\\n',\n",
              " '\\tNous nous sommes entretenues.\\n',\n",
              " '\\tNous nous entretînmes.\\n',\n",
              " '\\tNous attendîmes.\\n',\n",
              " '\\tNous avons attendu.\\n',\n",
              " '\\tNous essayerons.\\n',\n",
              " '\\tNous tenterons.\\n',\n",
              " \"\\tNous l'emporterons.\\n\",\n",
              " '\\tNous gagnerons.\\n',\n",
              " '\\tNous avons chaud.\\n',\n",
              " '\\tNous sommes tristes.\\n',\n",
              " '\\tNous sommes timides.\\n',\n",
              " '\\tBien vu\\u202f!\\n',\n",
              " '\\tBien cuit\\u202f!\\n',\n",
              " '\\tÀ la bonne heure\\u202f!\\n',\n",
              " '\\tBien joué\\u202f!\\n',\n",
              " '\\tPas mal !\\n',\n",
              " '\\tQuoi d’autre\\u202f?\\n',\n",
              " \"\\tQuoi d'autre ?\\n\",\n",
              " '\\tÇa va ?\\n',\n",
              " '\\tQuoi de beau\\u202f?\\n',\n",
              " \"\\tQui s'en préoccupe\\u202f?\\n\",\n",
              " \"\\tQui s'en soucie\\u202f?\\n\",\n",
              " '\\tÀ qui ceci importe-t-il\\u202f?\\n',\n",
              " '\\tQui est-ce\\u202f?\\n',\n",
              " '\\tQui est-il\\u202f?\\n',\n",
              " '\\tQui est-ce\\u202f?\\n',\n",
              " '\\tQui est-il\\u202f?\\n',\n",
              " '\\tQui sait\\u202f?\\n',\n",
              " '\\tQui a parlé ?\\n',\n",
              " '\\tQui ira ?\\n',\n",
              " '\\tQui est malade ?\\n',\n",
              " '\\tMagnifique\\u202f!\\n',\n",
              " '\\tÉcrivez à Tom.\\n',\n",
              " '\\tTu conduis.\\n',\n",
              " '\\tToi, conduis !\\n',\n",
              " '\\tVous conduisez.\\n',\n",
              " \"\\tEspèce d'imbécile\\u202f!\\n\",\n",
              " \"\\tEspèce d'idiot\\u202f!\\n\",\n",
              " '\\tTu commences.\\n',\n",
              " '\\tVous commencez.\\n',\n",
              " '\\tTu as essayé.\\n',\n",
              " '\\tVous avez essayé.\\n',\n",
              " '\\tTout le monde à bord!\\n',\n",
              " '\\tSuis-je clair ?\\n',\n",
              " '\\tSuis-je claire ?\\n',\n",
              " '\\tSuis-je en train de mourir ?\\n',\n",
              " '\\tSuis-je en train de trépasser ?\\n',\n",
              " '\\tSuis-je en avance ?\\n',\n",
              " '\\tVous me mettez à la porte ?\\n',\n",
              " '\\tSuis-je le premier ?\\n',\n",
              " '\\tSuis-je la première ?\\n',\n",
              " '\\tSuis-je engagé ?\\n',\n",
              " '\\tSuis-je engagée ?\\n',\n",
              " '\\tAi-je raison ?\\n',\n",
              " '\\tSuis-je dans le vrai ?\\n',\n",
              " '\\tMe trompé-je\\u202f?\\n',\n",
              " '\\tAi-je tort ?\\n',\n",
              " '\\tEst-ce que je me trompe ?\\n',\n",
              " '\\tEst-ce que tu vas bien\\u202f?\\n',\n",
              " '\\tEs-tu levé ?\\n',\n",
              " '\\tEs-tu levée ?\\n',\n",
              " '\\tÊtes-vous levé ?\\n',\n",
              " '\\tÊtes-vous levée ?\\n',\n",
              " '\\tÊtes-vous levés ?\\n',\n",
              " '\\tÊtes-vous levées ?\\n',\n",
              " '\\tEs-tu debout ?\\n',\n",
              " '\\tÊtes-vous debout ?\\n',\n",
              " \"\\tDemandez à n'importe qui.\\n\",\n",
              " \"\\tDemande à n'importe qui.\\n\",\n",
              " '\\tDemande à quiconque !\\n',\n",
              " '\\tDemande alentour !\\n',\n",
              " '\\tDemandez alentour !\\n',\n",
              " '\\tSois prudent !\\n',\n",
              " '\\tSois prudente !\\n',\n",
              " '\\tSoyez prudent !\\n',\n",
              " '\\tSoyez prudente !\\n',\n",
              " '\\tSoyez prudents !\\n',\n",
              " '\\tSoyez prudentes !\\n',\n",
              " '\\tSois satisfait !\\n',\n",
              " '\\tSois satisfaite !\\n',\n",
              " '\\tSoyez satisfait !\\n',\n",
              " '\\tSoyez satisfaite !\\n',\n",
              " '\\tSoyez satisfaits !\\n',\n",
              " '\\tSoyez satisfaites !\\n',\n",
              " '\\tSoyez sérieux !\\n',\n",
              " '\\tSoyez sérieuse !\\n',\n",
              " '\\tSoyez sérieuses !\\n',\n",
              " '\\tSois sérieux !\\n',\n",
              " '\\tSois sérieuse !\\n',\n",
              " '\\tLes oiseaux chantent.\\n',\n",
              " '\\tSanté !\\n',\n",
              " '\\tPuis-je venir ?\\n',\n",
              " '\\tPuis-je aider\\u202f?\\n',\n",
              " '\\tPuis-je rester ?\\n',\n",
              " '\\tPorte-le.\\n',\n",
              " '\\tVérifie ça.\\n',\n",
              " '\\tRegarde ça.\\n',\n",
              " '\\tChoisis-en un.\\n',\n",
              " '\\tChoisis-en une.\\n',\n",
              " '\\tJoignez-vous à nous.\\n',\n",
              " '\\tEntre donc\\u202f!\\n',\n",
              " '\\tEntre.\\n',\n",
              " '\\tViens vite\\u202f!\\n',\n",
              " '\\tJouis vite\\u202f!\\n',\n",
              " '\\tVenez à moi.\\n',\n",
              " '\\tVenez à nous.\\n',\n",
              " '\\tArrête !\\n',\n",
              " '\\tArrêtez !\\n',\n",
              " '\\tAssez\\u202f!\\n',\n",
              " '\\tAvons-nous gagné ?\\n',\n",
              " '\\tEst-ce que je pue ?\\n',\n",
              " '\\tJe vous en prie, entrez\\u202f!\\n',\n",
              " '\\tLes hommes pleurent-ils ?\\n',\n",
              " '\\tTe tracasse pas.\\n',\n",
              " '\\tVous tracassez pas.\\n',\n",
              " '\\tNe te tracasse pas.\\n',\n",
              " '\\tNe vous tracassez pas.\\n',\n",
              " '\\tNe bouge pas !\\n',\n",
              " '\\tNe bougez pas !\\n',\n",
              " '\\tNe bougez pas.\\n',\n",
              " '\\tNe bouge pas.\\n',\n",
              " '\\tNe te précipite pas.\\n',\n",
              " '\\tNe vous précipitez pas.\\n',\n",
              " '\\tNe parle pas !\\n',\n",
              " '\\tNe parlez pas !\\n',\n",
              " \"\\tN'attends pas !\\n\",\n",
              " \"\\tN'attendez pas !\\n\",\n",
              " \"\\tLe devoir m'appelle.\\n\",\n",
              " '\\tLe plein.\\n',\n",
              " '\\tTrouve un emploi !\\n',\n",
              " '\\tTrouve un boulot !\\n',\n",
              " '\\tSuis-le\\u202f!\\n',\n",
              " '\\tSuivez-le\\u202f!\\n',\n",
              " '\\tOublie-le.\\n',\n",
              " '\\tOubliez-le.\\n',\n",
              " '\\tPardonnez-moi.\\n',\n",
              " '\\tAchète-toi une vie !\\n',\n",
              " '\\tVa au lit !\\n',\n",
              " '\\tAu lit !\\n',\n",
              " '\\tAllez au lit !\\n',\n",
              " '\\tLaisse tomber.\\n',\n",
              " '\\tAbandonne\\u202f!\\n',\n",
              " \"\\tVa t'échauffer !\\n\",\n",
              " '\\tAllez vous échauffer !\\n',\n",
              " '\\tIl a cédé.\\n',\n",
              " '\\tIl céda.\\n',\n",
              " '\\tIl a raccroché.\\n',\n",
              " '\\tIl raccrocha.\\n',\n",
              " '\\tIl a à faire.\\n',\n",
              " '\\tIl est ici\\u202f!\\n',\n",
              " '\\tIl est gentil.\\n',\n",
              " '\\tIl est en retard.\\n',\n",
              " '\\tIl est fainéant.\\n',\n",
              " '\\tIl est paresseux.\\n',\n",
              " '\\tIl est pauvre.\\n',\n",
              " '\\tIl est malade.\\n',\n",
              " '\\tIl a réussi.\\n',\n",
              " '\\tIl est Suisse.\\n',\n",
              " '\\tIl est Helvète.\\n',\n",
              " '\\tIl est ruiné.\\n',\n",
              " '\\tIl est fauché.\\n',\n",
              " '\\tIl est ivre.\\n',\n",
              " '\\tIl est soûl.\\n',\n",
              " '\\tIl est intelligent.\\n',\n",
              " '\\tIl est ici\\u202f!\\n',\n",
              " '\\tLe voilà.\\n',\n",
              " '\\tTenez.\\n',\n",
              " '\\tLe voici.\\n',\n",
              " \"\\tC'est parti !\\n\",\n",
              " '\\tOn est partis !\\n',\n",
              " '\\tTiens-toi tranquille !\\n',\n",
              " '\\tTenez-vous tranquille !\\n',\n",
              " \"\\tComme c'est charmant !\\n\",\n",
              " '\\tComment va le travail ?\\n',\n",
              " \"\\tDépêche-toi d'aller chez toi !\\n\",\n",
              " \"\\tDépêchez-vous d'aller chez vous !\\n\",\n",
              " '\\tJe suis un homme.\\n',\n",
              " '\\tJe suis humain.\\n',\n",
              " '\\tJe suis prêt.\\n',\n",
              " \"\\tJ'ai raison.\\n\",\n",
              " '\\tJe suis désolé.\\n',\n",
              " '\\tJe suis désolée.\\n',\n",
              " '\\tJe suis fatigué !\\n',\n",
              " '\\tJe suis crevé.\\n',\n",
              " \"\\tJe l'ai cassé.\\n\",\n",
              " \"\\tJe l'ai cassée.\\n\",\n",
              " \"\\tJe l'ai construit.\\n\",\n",
              " \"\\tJe l'ai construite.\\n\",\n",
              " '\\tJe peux venir.\\n',\n",
              " '\\tJe sais cuisiner.\\n',\n",
              " '\\tJe peux sauter.\\n',\n",
              " '\\tJe sais lire.\\n',\n",
              " '\\tJe peux lire.\\n',\n",
              " '\\tJe sais chanter.\\n',\n",
              " '\\tJe peux rester.\\n',\n",
              " '\\tJe sais nager.\\n',\n",
              " '\\tJe peux attendre.\\n',\n",
              " '\\tJe peux marcher.\\n',\n",
              " '\\tJe ne peux pas y aller.\\n',\n",
              " \"\\tJe ne peux pas m'y rendre.\\n\",\n",
              " '\\tJe ne peux pas partir.\\n',\n",
              " \"\\tJ'ai fait cela.\\n\",\n",
              " \"\\tC'est moi qui l'ai fait.\\n\",\n",
              " \"\\tJe ne suis pas d'accord.\\n\",\n",
              " '\\tJe me fais vraiment du souci.\\n',\n",
              " \"\\tJ'en doute.\\n\",\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbdxgjAOOnEf",
        "colab_type": "text"
      },
      "source": [
        "# **English to German Translation**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdzwAhduO2R4",
        "colab_type": "code",
        "outputId": "c556b820-714a-4e63-99de-44b24da546f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('deu.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "deu_sent = []\n",
        "eng_chars = set()\n",
        "deu_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and german sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    deu_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    deu_sent.append(deu_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in deu_line:\n",
        "        if (ch not in deu_chars):\n",
        "            deu_chars.add(ch)\n",
        "\n",
        "deu_chars = sorted(list(deu_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each german character - key is index and value is german character\n",
        "deu_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get german character given its index - key is german character and value is index\n",
        "deu_char_to_index_dict = {}\n",
        "for k, v in enumerate(deu_chars):\n",
        "    deu_index_to_char_dict[k] = v\n",
        "    deu_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_deu_sent = max([len(line) for line in deu_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_deu_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_deu_sentences = np.zeros(shape = (nb_samples,max_len_deu_sent,len(deu_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_deu_sent, len(deu_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(deu_sent[i]):\n",
        "        tokenized_deu_sentences[i,k,deu_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,deu_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(deu_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(deu_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_deu_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(deu_chars)))\n",
        "    target_seq[0, 0, deu_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent1 = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_deu_char = deu_index_to_char_dict[max_val_index]\n",
        "        translated_sent1 += sampled_deu_char\n",
        "        \n",
        "        if ( (sampled_deu_char == '\\n') or (len(translated_sent1) > max_len_deu_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(deu_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.1770 - val_loss: 1.1526\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.9076 - val_loss: 0.9286\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.7666 - val_loss: 0.8313\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.6991 - val_loss: 0.7623\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.6502 - val_loss: 0.7136\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.6130 - val_loss: 0.6935\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5806 - val_loss: 0.6610\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5548 - val_loss: 0.6416\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5323 - val_loss: 0.6214\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5114 - val_loss: 0.6041\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4928 - val_loss: 0.5870\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4760 - val_loss: 0.5828\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4603 - val_loss: 0.5713\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4456 - val_loss: 0.5567\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4316 - val_loss: 0.5550\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4183 - val_loss: 0.5458\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4056 - val_loss: 0.5431\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3934 - val_loss: 0.5397\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3819 - val_loss: 0.5260\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3701 - val_loss: 0.5282\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3600 - val_loss: 0.5247\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3497 - val_loss: 0.5279\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3397 - val_loss: 0.5297\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3300 - val_loss: 0.5231\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3209 - val_loss: 0.5262\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3122 - val_loss: 0.5263\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3036 - val_loss: 0.5220\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2951 - val_loss: 0.5297\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2870 - val_loss: 0.5309\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2796 - val_loss: 0.5293\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2724 - val_loss: 0.5343\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2649 - val_loss: 0.5356\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2584 - val_loss: 0.5323\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2515 - val_loss: 0.5445\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2452 - val_loss: 0.5427\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2391 - val_loss: 0.5493\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2328 - val_loss: 0.5459\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2269 - val_loss: 0.5528\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2218 - val_loss: 0.5587\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2164 - val_loss: 0.5630\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2110 - val_loss: 0.5657\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2062 - val_loss: 0.5700\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2016 - val_loss: 0.5735\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1966 - val_loss: 0.5789\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1920 - val_loss: 0.5853\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1878 - val_loss: 0.5943\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1837 - val_loss: 0.5945\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1802 - val_loss: 0.5965\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1761 - val_loss: 0.6020\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1723 - val_loss: 0.6054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB58Ud5gTWBS",
        "colab_type": "code",
        "outputId": "b1d0cec5-b12b-4177-cc09-8f43cc4d6d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent1 = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Hallo!\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Hallo!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Mach wei!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Seid spazier!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Seid spazier!\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Funderback mal Geldinen!\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Hallo!\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Hallo!\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Stoppf!\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Verkrümele dich!\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Nur zu!\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Hallo!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZTfXILsQFzY",
        "colab_type": "code",
        "outputId": "4cfd8e08-8dc9-48cf-eac2-6c3e6ecc055d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "deu_sent"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\tHallo!\\n',\n",
              " '\\tGrüß Gott!\\n',\n",
              " '\\tLauf!\\n',\n",
              " '\\tPotzdonner!\\n',\n",
              " '\\tDonnerwetter!\\n',\n",
              " '\\tFeuer!\\n',\n",
              " '\\tHilfe!\\n',\n",
              " '\\tZu Hülf!\\n',\n",
              " '\\tStopp!\\n',\n",
              " '\\tWarte!\\n',\n",
              " '\\tMach weiter.\\n',\n",
              " '\\tHallo!\\n',\n",
              " '\\tIch rannte.\\n',\n",
              " '\\tIch verstehe.\\n',\n",
              " '\\tAha.\\n',\n",
              " '\\tIch probiere es.\\n',\n",
              " '\\tIch hab gewonnen!\\n',\n",
              " '\\tIch habe gewonnen!\\n',\n",
              " '\\tLächeln!\\n',\n",
              " '\\tZum Wohl!\\n',\n",
              " '\\tIss auf.\\n',\n",
              " '\\tKeine Bewegung!\\n',\n",
              " '\\tStehenbleiben!\\n',\n",
              " '\\tKapiert?\\n',\n",
              " '\\tVerstanden?\\n',\n",
              " '\\tEinverstanden?\\n',\n",
              " '\\tEr rannte.\\n',\n",
              " '\\tEr lief.\\n',\n",
              " '\\tMach mit!\\n',\n",
              " '\\tDrück mich!\\n',\n",
              " '\\tNimm mich in den Arm!\\n',\n",
              " '\\tUmarme mich!\\n',\n",
              " '\\tIch fiel.\\n',\n",
              " '\\tIch fiel hin.\\n',\n",
              " '\\tIch stürzte.\\n',\n",
              " '\\tIch bin hingefallen.\\n',\n",
              " '\\tIch bin gestürzt.\\n',\n",
              " '\\tIch weiß.\\n',\n",
              " '\\tIch habe gelogen.\\n',\n",
              " '\\tIch habe verloren.\\n',\n",
              " '\\tIch habe bezahlt.\\n',\n",
              " '\\tIch zahlte.\\n',\n",
              " '\\tIch sang.\\n',\n",
              " '\\tIch schwimme.\\n',\n",
              " '\\tIch bin 19 Jahre alt.\\n',\n",
              " '\\tIch bin 19.\\n',\n",
              " \"\\tMir geht's gut.\\n\",\n",
              " '\\tEs geht mir gut.\\n',\n",
              " '\\tIch bin wach.\\n',\n",
              " '\\tIch bin auf.\\n',\n",
              " '\\tUnmöglich!\\n',\n",
              " '\\tDas kommt nicht in Frage!\\n',\n",
              " '\\tDas gibt’s doch nicht!\\n',\n",
              " '\\tAusgeschlossen!\\n',\n",
              " '\\tIn keinster Weise!\\n',\n",
              " '\\tWirklich?\\n',\n",
              " '\\tEcht?\\n',\n",
              " '\\tIm Ernst?\\n',\n",
              " '\\tDanke!\\n',\n",
              " '\\tVersuch’s!\\n',\n",
              " '\\tWir versuchen es.\\n',\n",
              " '\\tWir haben gewonnen.\\n',\n",
              " '\\tWarum ich?\\n',\n",
              " '\\tFrag Tom!\\n',\n",
              " '\\tFragen Sie Tom!\\n',\n",
              " '\\tFragt Tom!\\n',\n",
              " '\\tFantastisch!\\n',\n",
              " '\\tEntspann dich!\\n',\n",
              " '\\tSei nicht ungerecht!\\n',\n",
              " '\\tSei fair!\\n',\n",
              " '\\tSei nett!\\n',\n",
              " '\\tSeien Sie nett!\\n',\n",
              " '\\tGeh weg!\\n',\n",
              " '\\tHau ab!\\n',\n",
              " '\\tVerschwinde!\\n',\n",
              " '\\tVerdufte!\\n',\n",
              " '\\tMach dich fort!\\n',\n",
              " '\\tZieh Leine!\\n',\n",
              " '\\tMach dich vom Acker!\\n',\n",
              " '\\tVerzieh dich!\\n',\n",
              " '\\tVerkrümele dich!\\n',\n",
              " '\\tTroll dich!\\n',\n",
              " '\\tZisch ab!\\n',\n",
              " '\\tPack dich!\\n',\n",
              " '\\tMach ’ne Fliege!\\n',\n",
              " '\\tSchwirr ab!\\n',\n",
              " '\\tMach die Sause!\\n',\n",
              " '\\tScher dich weg!\\n',\n",
              " '\\tScher dich fort!\\n',\n",
              " '\\tRuf mich an.\\n',\n",
              " '\\tKomm herein.\\n',\n",
              " '\\tHerein!\\n',\n",
              " '\\tKomm!\\n',\n",
              " '\\tKommt!\\n',\n",
              " '\\tMach schon!\\n',\n",
              " '\\tMacht schon!\\n',\n",
              " '\\tKomm schon!\\n',\n",
              " '\\tHol Tom.\\n',\n",
              " '\\tRaus!\\n',\n",
              " '\\tGeht raus!\\n',\n",
              " '\\tGeh raus.\\n',\n",
              " '\\tGeht raus!\\n',\n",
              " '\\tGeh weg!\\n',\n",
              " '\\tHau ab!\\n',\n",
              " '\\tVerschwinde!\\n',\n",
              " '\\tVerdufte!\\n',\n",
              " '\\tMach dich fort!\\n',\n",
              " '\\tZieh Leine!\\n',\n",
              " '\\tMach dich vom Acker!\\n',\n",
              " '\\tVerzieh dich!\\n',\n",
              " '\\tVerkrümele dich!\\n',\n",
              " '\\tTroll dich!\\n',\n",
              " '\\tZisch ab!\\n',\n",
              " '\\tPack dich!\\n',\n",
              " '\\tMach ’ne Fliege!\\n',\n",
              " '\\tSchwirr ab!\\n',\n",
              " '\\tMach die Sause!\\n',\n",
              " '\\tScher dich weg!\\n',\n",
              " '\\tScher dich fort!\\n',\n",
              " '\\tGeh weg!\\n',\n",
              " '\\tVerpiss dich!\\n',\n",
              " '\\tHau ab!\\n',\n",
              " '\\tVerschwinde!\\n',\n",
              " '\\tVerdufte!\\n',\n",
              " '\\tMach dich fort!\\n',\n",
              " '\\tZieh Leine!\\n',\n",
              " '\\tMach dich vom Acker!\\n',\n",
              " '\\tVerzieh dich!\\n',\n",
              " '\\tVerkrümele dich!\\n',\n",
              " '\\tTroll dich!\\n',\n",
              " '\\tZisch ab!\\n',\n",
              " '\\tPack dich!\\n',\n",
              " '\\tMach ’ne Fliege!\\n',\n",
              " '\\tSchwirr ab!\\n',\n",
              " '\\tMach die Sause!\\n',\n",
              " '\\tScher dich weg!\\n',\n",
              " '\\tScher dich fort!\\n',\n",
              " '\\tGehen Sie weg.\\n',\n",
              " '\\tGeh nach Hause.\\n',\n",
              " '\\tGeh heim.\\n',\n",
              " '\\tAuf Wiedersehen!\\n',\n",
              " '\\tLeb wohl!\\n',\n",
              " '\\tTschüss!\\n',\n",
              " '\\tNicht nachlassen!\\n',\n",
              " '\\tEr kam.\\n',\n",
              " '\\tEr rennt.\\n',\n",
              " '\\tEr läuft.\\n',\n",
              " '\\tHilf mir.\\n',\n",
              " '\\tHilf uns!\\n',\n",
              " '\\tHelft uns!\\n',\n",
              " '\\tHelfen Sie uns!\\n',\n",
              " '\\tHallo, Tom!\\n',\n",
              " '\\tSchlage Tom!\\n',\n",
              " '\\tSchlagt Tom!\\n',\n",
              " '\\tSchlagen Sie Tom!\\n',\n",
              " '\\tWarten Sie kurz!\\n',\n",
              " '\\tUmarme Tom!\\n',\n",
              " '\\tUmarmen Sie Tom!\\n',\n",
              " '\\tUmarmt Tom!\\n',\n",
              " '\\tDrückt Tom!\\n',\n",
              " '\\tDrücken Sie Tom!\\n',\n",
              " '\\tDrück Tom!\\n',\n",
              " '\\tIch bin einverstanden.\\n',\n",
              " '\\tIch weinte.\\n',\n",
              " '\\tIch habe geweint.\\n',\n",
              " '\\tIch schnarche.\\n',\n",
              " '\\tIch gehe.\\n',\n",
              " '\\tIch bin schlecht.\\n',\n",
              " '\\tIch bin fett.\\n',\n",
              " '\\tIch bin dick.\\n',\n",
              " '\\tIch wurde getroffen!\\n',\n",
              " '\\tIch bin neu.\\n',\n",
              " '\\tIch bin alt.\\n',\n",
              " '\\tIch bin traurig.\\n',\n",
              " '\\tIch bin schüchtern.\\n',\n",
              " '\\tIch bin nass.\\n',\n",
              " '\\tEs ist in Ordnung.\\n',\n",
              " \"\\tIch bin's.\\n\",\n",
              " \"\\tIch bin's.\\n\",\n",
              " '\\tBehalt es!\\n',\n",
              " '\\tBehalt ihn!\\n',\n",
              " '\\tBehalt sie!\\n',\n",
              " '\\tBehalten Sie ihn!\\n',\n",
              " '\\tBehalten Sie sie!\\n',\n",
              " '\\tBehalten Sie es!\\n',\n",
              " '\\tBehaltet ihn!\\n',\n",
              " '\\tBehaltet sie!\\n',\n",
              " '\\tBehaltet es!\\n',\n",
              " '\\tKüsst mich.\\n',\n",
              " '\\tSchließ es ab.\\n',\n",
              " '\\tSchließe sie ab.\\n',\n",
              " '\\tSchließ ihn ab.\\n',\n",
              " '\\tPerfekt!\\n',\n",
              " '\\tZieh dran.\\n',\n",
              " '\\tDrück drauf.\\n',\n",
              " '\\tWir sehen uns.\\n',\n",
              " \"\\tZeig's mir!\\n\",\n",
              " \"\\tHalt's Maul!\\n\",\n",
              " '\\tBis später!\\n',\n",
              " '\\tHör auf.\\n',\n",
              " '\\tNimm es.\\n',\n",
              " '\\tNehmt es.\\n',\n",
              " '\\tNehmen Sie es.\\n',\n",
              " '\\tTom aß.\\n',\n",
              " '\\tTom hat gegessen.\\n',\n",
              " '\\tTom rannte.\\n',\n",
              " '\\tTom ist gerannt.\\n',\n",
              " '\\tTom hat gewonnen.\\n',\n",
              " '\\tWarte mal!\\n',\n",
              " '\\tWarten Sie mal!\\n',\n",
              " '\\tWartet mal!\\n',\n",
              " '\\tWach auf!\\n',\n",
              " '\\tWachen Sie auf!\\n',\n",
              " '\\tWach auf!\\n',\n",
              " '\\tWachen Sie auf!\\n',\n",
              " '\\tWir haben verloren.\\n',\n",
              " '\\tWillkommen!\\n',\n",
              " '\\tWer hat gegessen?\\n',\n",
              " '\\tWer aß?\\n',\n",
              " '\\tWer rannte?\\n',\n",
              " '\\tWer ist gerannt?\\n',\n",
              " '\\tDu läufst.\\n',\n",
              " '\\tSie laufen.\\n',\n",
              " '\\tDu hast gewonnen.\\n',\n",
              " '\\tBin ich dick?\\n',\n",
              " '\\tFrag sie.\\n',\n",
              " '\\tKomm nicht näher!\\n',\n",
              " '\\tSei ein Mann!\\n',\n",
              " '\\tSei tapfer!\\n',\n",
              " '\\tSeien Sie tapfer!\\n',\n",
              " '\\tSeid tapfer!\\n',\n",
              " '\\tFassen Sie sich kurz.\\n',\n",
              " '\\tFass dich kurz.\\n',\n",
              " '\\tFasst euch kurz.\\n',\n",
              " '\\tRuf Tom an!\\n',\n",
              " '\\tRufe Tom an!\\n',\n",
              " '\\tRufen Sie Tom an!\\n',\n",
              " '\\tRuft Tom an!\\n',\n",
              " '\\tKann ich gehen?\\n',\n",
              " '\\tKopf hoch!\\n',\n",
              " '\\tReg dich ab!\\n',\n",
              " '\\tLeg ihm Handschellen an.\\n',\n",
              " '\\tLegen Sie ihm Handschellen an.\\n',\n",
              " '\\tGeh nicht.\\n',\n",
              " '\\tFinde Tom.\\n',\n",
              " '\\tFindet Tom.\\n',\n",
              " '\\tFinden Sie Tom.\\n',\n",
              " '\\tBeheben Sie das.\\n',\n",
              " '\\tBehebe das.\\n',\n",
              " '\\tRepariere das.\\n',\n",
              " '\\tReparieren Sie das.\\n',\n",
              " '\\tGeh weg!\\n',\n",
              " '\\tVerpiss dich!\\n',\n",
              " '\\tHau ab!\\n',\n",
              " '\\tVerschwinde!\\n',\n",
              " '\\tVerdufte!\\n',\n",
              " '\\tMach dich fort!\\n',\n",
              " '\\tZieh Leine!\\n',\n",
              " '\\tMach dich vom Acker!\\n',\n",
              " '\\tVerzieh dich!\\n',\n",
              " '\\tVerkrümele dich!\\n',\n",
              " '\\tTroll dich!\\n',\n",
              " '\\tZisch ab!\\n',\n",
              " '\\tPack dich!\\n',\n",
              " '\\tMach ’ne Fliege!\\n',\n",
              " '\\tSchwirr ab!\\n',\n",
              " '\\tMach die Sause!\\n',\n",
              " '\\tScher dich weg!\\n',\n",
              " '\\tScher dich fort!\\n',\n",
              " '\\tRunter!\\n',\n",
              " '\\tHinlegen!\\n',\n",
              " '\\tIn Deckung!\\n',\n",
              " '\\tKomm runter.\\n',\n",
              " '\\tKommen Sie runter.\\n',\n",
              " '\\tGeh weg!\\n',\n",
              " '\\tVerpiss dich!\\n',\n",
              " '\\tHau ab!\\n',\n",
              " '\\tVerschwinde!\\n',\n",
              " '\\tVerdufte!\\n',\n",
              " '\\tMach dich fort!\\n',\n",
              " '\\tZieh Leine!\\n',\n",
              " '\\tMach dich vom Acker!\\n',\n",
              " '\\tVerzieh dich!\\n',\n",
              " '\\tVerkrümele dich!\\n',\n",
              " '\\tTroll dich!\\n',\n",
              " '\\tZisch ab!\\n',\n",
              " '\\tPack dich!\\n',\n",
              " '\\tMach ’ne Fliege!\\n',\n",
              " '\\tSchwirr ab!\\n',\n",
              " '\\tMach die Sause!\\n',\n",
              " '\\tScher dich weg!\\n',\n",
              " '\\tScher dich fort!\\n',\n",
              " '\\tJetzt mal ernsthaft!\\n',\n",
              " '\\tNur zu!\\n',\n",
              " '\\tMach weiter!\\n',\n",
              " '\\tHol Tom.\\n',\n",
              " '\\tGreif ihn dir!\\n',\n",
              " '\\tViel Vergnügen!\\n',\n",
              " '\\tViel Spaß!\\n',\n",
              " '\\tFeier schön!\\n',\n",
              " '\\tEr sprach.\\n',\n",
              " '\\tEr versucht es.\\n',\n",
              " '\\tHilf Tom!\\n',\n",
              " '\\tHelft Tom!\\n',\n",
              " '\\tHelfen Sie Tom!\\n',\n",
              " '\\tWas ist das nicht süß!\\n',\n",
              " '\\tWie süß!\\n',\n",
              " '\\tWie tief?\\n',\n",
              " '\\tWie schön!\\n',\n",
              " '\\tTu mir den Gefallen.\\n',\n",
              " '\\tTun Sie mir den Gefallen.\\n',\n",
              " '\\tBeeil dich!\\n',\n",
              " '\\tBeeil dich.\\n',\n",
              " '\\tMach hin!\\n',\n",
              " '\\tIch kann gehen.\\n',\n",
              " '\\tIch habe es geschafft.\\n',\n",
              " \"\\tIch hab's gemacht.\\n\",\n",
              " '\\tIch komme klar.\\n',\n",
              " '\\tIch komme zurecht.\\n',\n",
              " '\\tIch komme über die Runden.\\n',\n",
              " '\\tIch verstehe.\\n',\n",
              " '\\tIch habe es verstanden.\\n',\n",
              " '\\tIch habe es bekommen.\\n',\n",
              " \"\\tIch hab's.\\n\",\n",
              " '\\tIch half.\\n',\n",
              " '\\tIch habe geholfen.\\n',\n",
              " '\\tIch bin gesprungen.\\n',\n",
              " '\\tIch weigere mich.\\n',\n",
              " '\\tIch trete zurück.\\n',\n",
              " '\\tIch rasierte mich.\\n',\n",
              " '\\tIch habe mich rasiert.\\n',\n",
              " '\\tIch lächelte.\\n',\n",
              " '\\tIch blieb.\\n',\n",
              " '\\tIch bin dageblieben.\\n',\n",
              " '\\tIch benutze es.\\n',\n",
              " '\\tIch habe gewartet.\\n',\n",
              " '\\tIch gähnte.\\n',\n",
              " '\\tIch habe gegähnt.\\n',\n",
              " '\\tIch werde zahlen.\\n',\n",
              " '\\tIch werde es versuchen.\\n',\n",
              " '\\tIch bin DJ.\\n',\n",
              " '\\tIch bin wieder da.\\n',\n",
              " '\\tIch bin glatzköpfig.\\n',\n",
              " '\\tIch habe eine Glatze.\\n',\n",
              " '\\tIch bin beschäftigt.\\n',\n",
              " '\\tIch habe zu tun.\\n',\n",
              " '\\tIch bin taub.\\n',\n",
              " '\\tIch bin gerecht.\\n',\n",
              " \"\\tMir geht's gut.\\n\",\n",
              " '\\tMir geht es gut.\\n',\n",
              " '\\tEs geht mir gut.\\n',\n",
              " '\\tIch bin frei.\\n',\n",
              " '\\tIch bin satt.\\n',\n",
              " '\\tIch bin dabei.\\n',\n",
              " '\\tIch mache mit.\\n',\n",
              " \"\\tMir geht's gut.\\n\",\n",
              " '\\tIch bin hier.\\n',\n",
              " '\\tIch bin zu Hause.\\n',\n",
              " '\\tIch komme zu spät.\\n',\n",
              " '\\tIch habe mich verirrt.\\n',\n",
              " '\\tIch bin gemein.\\n',\n",
              " '\\tIch bin der Nächste.\\n',\n",
              " '\\tEs geht mir gut.\\n',\n",
              " '\\tIch bin reich.\\n',\n",
              " '\\tIch bin sicher.\\n',\n",
              " '\\tIch bin krank!\\n',\n",
              " '\\tIch bin sicher.\\n',\n",
              " '\\tIch bin groß.\\n',\n",
              " '\\tIch bin dünn.\\n',\n",
              " '\\tIch bin ordentlich.\\n',\n",
              " '\\tIch bin hässlich.\\n',\n",
              " '\\tIch bin schwach.\\n',\n",
              " '\\tMir geht es gut.\\n',\n",
              " '\\tDas hilft.\\n',\n",
              " '\\tEs tut weh.\\n',\n",
              " '\\tEs schmerzt.\\n',\n",
              " '\\tEs klappt.\\n',\n",
              " '\\tEs funktioniert.\\n',\n",
              " '\\tEs ist Tom.\\n',\n",
              " '\\tEs ist seins.\\n',\n",
              " '\\tEs ist heiß.\\n',\n",
              " '\\tSie ist heiß.\\n',\n",
              " '\\tEr ist heiß.\\n',\n",
              " '\\tEs ist neu.\\n',\n",
              " '\\tEs ist traurig.\\n',\n",
              " '\\tEintritt verboten!\\n',\n",
              " '\\tKomm nicht herein.\\n',\n",
              " '\\tKein Zutritt.\\n',\n",
              " '\\tKüsse Tom!\\n',\n",
              " '\\tKüssen Sie Tom!\\n',\n",
              " '\\tKüsst Tom!\\n',\n",
              " '\\tGeh weg!\\n',\n",
              " '\\tGehen Sie weg.\\n',\n",
              " '\\tLass uns allein.\\n',\n",
              " '\\tLassen Sie uns allein.\\n',\n",
              " '\\tLass uns gehen!\\n',\n",
              " \"\\tAuf geht's!\\n\",\n",
              " '\\tGehen wir!\\n',\n",
              " '\\tLasst uns gehen.\\n',\n",
              " '\\tLass uns losgehen.\\n',\n",
              " '\\tLasst uns losgehen.\\n',\n",
              " '\\tAuf, auf!\\n',\n",
              " '\\tVorsicht!\\n',\n",
              " '\\tHeirate mich.\\n',\n",
              " '\\tDarf ich gehen?\\n',\n",
              " '\\tRette Tom!\\n',\n",
              " '\\tRettet Tom!\\n',\n",
              " '\\tRetten Sie Tom!\\n',\n",
              " '\\tSie kam.\\n',\n",
              " '\\tSie log.\\n',\n",
              " '\\tSie rennt.\\n',\n",
              " '\\tSetz dich!\\n',\n",
              " '\\tSetzen Sie sich!\\n',\n",
              " '\\tSetz dich!\\n',\n",
              " '\\tSprich lauter!\\n',\n",
              " '\\tStehen Sie auf!\\n',\n",
              " '\\tSteht auf!\\n',\n",
              " '\\tStehe auf!\\n',\n",
              " '\\tHalte Tom auf!\\n',\n",
              " '\\tHalten Sie Tom auf!\\n',\n",
              " '\\tHaltet Tom auf!\\n',\n",
              " '\\tNimm Tom.\\n',\n",
              " '\\tProbier es mal.\\n',\n",
              " '\\tSag es Tom.\\n',\n",
              " '\\tSagen Sie es Tom.\\n',\n",
              " '\\tErzähl es Tom.\\n',\n",
              " '\\tErzählen Sie Tom davon.\\n',\n",
              " '\\tSagt es Tom.\\n',\n",
              " '\\tHervorragend!\\n',\n",
              " '\\tSagenhaft!\\n',\n",
              " '\\tWunderbar!\\n',\n",
              " '\\tSie haben gewonnen.\\n',\n",
              " '\\tTom kam.\\n',\n",
              " '\\tTom ist gekommen.\\n',\n",
              " '\\tTom ist gestorben.\\n',\n",
              " '\\tTom starb.\\n',\n",
              " '\\tTom fiel.\\n',\n",
              " '\\tTom ist gefallen.\\n',\n",
              " '\\tTom wusste es.\\n',\n",
              " '\\tTom hat es gewusst.\\n',\n",
              " '\\tTom wusste Bescheid.\\n',\n",
              " '\\tTom ist gegangen.\\n',\n",
              " '\\tTom ging.\\n',\n",
              " '\\tTom log.\\n',\n",
              " '\\tTom lügt.\\n',\n",
              " '\\tTom hat verloren.\\n',\n",
              " '\\tTom hat gezahlt.\\n',\n",
              " '\\tTom hat aufgehört.\\n',\n",
              " '\\tTom schwamm.\\n',\n",
              " '\\tTom ist geschwommen.\\n',\n",
              " '\\tTom weinte.\\n',\n",
              " '\\tTom ist auf.\\n',\n",
              " '\\tZu spät.\\n',\n",
              " '\\tFass es an.\\n',\n",
              " '\\tVertraue mir.\\n',\n",
              " '\\tVertraut mir!\\n',\n",
              " '\\tVertrauen Sie mir!\\n',\n",
              " '\\tVersuch es richtig!\\n',\n",
              " '\\tNimm das hier!\\n',\n",
              " '\\tWarnen Sie Tom.\\n',\n",
              " '\\tWarne Tom.\\n',\n",
              " '\\tSchau mir zu.\\n',\n",
              " '\\tSchauen Sie mir zu.\\n',\n",
              " '\\tBeobachte uns.\\n',\n",
              " '\\tBeobachten Sie uns.\\n',\n",
              " '\\tSchau uns zu.\\n',\n",
              " '\\tSchaut uns zu.\\n',\n",
              " '\\tSchauen Sie uns zu.\\n',\n",
              " '\\tBeobachtet uns.\\n',\n",
              " '\\tWir stimmen zu.\\n',\n",
              " '\\tWir sind einverstanden.\\n',\n",
              " '\\tWir haben es versucht.\\n',\n",
              " '\\tWir versuchten es.\\n',\n",
              " '\\tWir werden gehen.\\n',\n",
              " '\\tWir sind in Ordnung.\\n',\n",
              " '\\tWozu?\\n',\n",
              " '\\tWas für ein Spaß!\\n',\n",
              " '\\tWer bin ich?\\n',\n",
              " '\\tWer ist gekommen?\\n',\n",
              " '\\tWer kam?\\n',\n",
              " '\\tWer ist gestorben?\\n',\n",
              " '\\tWer fiel?\\n',\n",
              " '\\tWer ist gefallen?\\n',\n",
              " '\\tWer hat aufgehört?\\n',\n",
              " '\\tWer ist ausgeschieden?\\n',\n",
              " '\\tWer schwamm?\\n',\n",
              " '\\tWer ist geschwommen?\\n',\n",
              " '\\tWer ist er?\\n',\n",
              " '\\tSchreib mir!\\n',\n",
              " '\\tSchreiben Sie mir!\\n',\n",
              " '\\tSchreibt mir!\\n',\n",
              " '\\tSie haben verloren.\\n',\n",
              " '\\tDu hast verloren.\\n',\n",
              " '\\tIhr habt verloren.\\n',\n",
              " '\\tZielen. Feuer!\\n',\n",
              " '\\tBin ich tot?\\n',\n",
              " '\\tBin ich zu spät?\\n',\n",
              " '\\tAntworten Sie mir.\\n',\n",
              " '\\tVögel fliegen.\\n',\n",
              " '\\tGesundheit.\\n',\n",
              " '\\tRuf zuhause an!\\n',\n",
              " '\\tRufen Sie zuhause an!\\n',\n",
              " '\\tBeruhige dich!\\n',\n",
              " '\\tBeruhige dich!\\n',\n",
              " '\\tBeruhigen Sie sich!\\n',\n",
              " '\\tBeruhigen Sie sich.\\n',\n",
              " '\\tKann ich essen?\\n',\n",
              " '\\tKönnen wir gehen?\\n',\n",
              " '\\tFang Tom!\\n',\n",
              " '\\tFange Tom!\\n',\n",
              " '\\tFangen Sie Tom!\\n',\n",
              " '\\tFangt Tom!\\n',\n",
              " '\\tEntspann dich.\\n',\n",
              " '\\tKomm wieder!\\n',\n",
              " '\\tKomm hierher.\\n',\n",
              " '\\tKomm her!\\n',\n",
              " '\\tKommt heim!\\n',\n",
              " '\\tKomm heim!\\n',\n",
              " '\\tKomm hierher!\\n',\n",
              " '\\tKomm her!\\n',\n",
              " '\\tKomm bald.\\n',\n",
              " '\\tKommen Sie bald.\\n',\n",
              " '\\tBeruhige dich!\\n',\n",
              " '\\tHabe ich gewonnen?\\n',\n",
              " '\\tMach es jetzt!\\n',\n",
              " '\\tHunde bellen.\\n',\n",
              " '\\tFrag nicht.\\n',\n",
              " '\\tWeine nicht!\\n',\n",
              " '\\tWeint nicht.\\n',\n",
              " '\\tWeinen Sie nicht.\\n',\n",
              " '\\tWeinen Sie nicht!\\n',\n",
              " '\\tWeint nicht!\\n',\n",
              " '\\tStirb nicht!\\n',\n",
              " '\\tLüge nicht.\\n',\n",
              " '\\tLauf nicht.\\n',\n",
              " '\\tEs tut mir leid.\\n',\n",
              " '\\tEntschuldigung!\\n',\n",
              " '\\tEntschuldigung.\\n',\n",
              " '\\tEntschuldigen Sie!\\n',\n",
              " '\\tFantastisch!\\n',\n",
              " '\\tGanz toll!\\n',\n",
              " '\\tFühl mal.\\n',\n",
              " '\\tFolge mir.\\n',\n",
              " '\\tVergiss es!\\n',\n",
              " '\\tDaraus wird nichts.\\n',\n",
              " '\\tVergiss es.\\n',\n",
              " '\\tDas kannst du knicken.\\n',\n",
              " '\\tNur zu!\\n',\n",
              " '\\tTue es.\\n',\n",
              " \"\\tHol's dir!\\n\",\n",
              " '\\tGeh rein!\\n',\n",
              " '\\tKomm rein.\\n',\n",
              " '\\tGeh schlafen!\\n',\n",
              " '\\tGeh ins Bett!\\n',\n",
              " '\\tGeht ins Bett!\\n',\n",
              " '\\tLegt euch schlafen!\\n',\n",
              " '\\tHände weg!\\n',\n",
              " '\\tNimm dir davon.\\n',\n",
              " '\\tNehmen Sie davon.\\n',\n",
              " '\\tEr ist krank.\\n',\n",
              " '\\tEr ist alt.\\n',\n",
              " '\\tEr lächelte.\\n',\n",
              " '\\tEr ist DJ.\\n',\n",
              " '\\tEr ist Plattenaufleger.\\n',\n",
              " '\\tEr ist schnell.\\n',\n",
              " '\\tEr ist gut.\\n',\n",
              " '\\tEr ist träge.\\n',\n",
              " '\\tEr ist reich.\\n',\n",
              " '\\tHier bin ich.\\n',\n",
              " '\\tHier sind fünf Dollar.\\n',\n",
              " '\\tNicht schießen!\\n',\n",
              " '\\tHalt das!\\n',\n",
              " '\\tHalt das mal.\\n',\n",
              " '\\tHalten Sie das!\\n',\n",
              " '\\tHaltet das!\\n',\n",
              " '\\tSchrecklich!\\n',\n",
              " '\\tWie ist die Lage?\\n',\n",
              " '\\tWie seltsam!\\n',\n",
              " '\\tLass Tom doch seinen Willen.\\n',\n",
              " '\\tLasst Tom doch seinen Willen.\\n',\n",
              " '\\tIch habe mich verirrt.\\n',\n",
              " '\\tEs geht mir gut.\\n',\n",
              " '\\tIch bin sicher.\\n',\n",
              " '\\tIch bin groß.\\n',\n",
              " '\\tMir geht es gut.\\n',\n",
              " '\\tEs geht mir gut.\\n',\n",
              " '\\tIch kann fliegen.\\n',\n",
              " '\\tIch kann laufen.\\n',\n",
              " '\\tIch kann rennen.\\n',\n",
              " '\\tIch kann Ski fahren.\\n',\n",
              " '\\tIch kann gewinnen.\\n',\n",
              " '\\tIch habe geschummelt.\\n',\n",
              " '\\tIch klatschte.\\n',\n",
              " '\\tIch wurde ohnmächtig.\\n',\n",
              " '\\tIch fiel in Ohnmacht.\\n',\n",
              " '\\tIch fürchte, ja.\\n',\n",
              " '\\tIch habe aufgegeben.\\n',\n",
              " '\\tIch gab auf.\\n',\n",
              " '\\tIch verstehe, was du meinst.\\n',\n",
              " '\\tIch kicherte.\\n',\n",
              " '\\tIch gebe nach.\\n',\n",
              " '\\tIch füge mich.\\n',\n",
              " '\\tIch wurde wütend.\\n',\n",
              " '\\tIch wurde böse.\\n',\n",
              " '\\tIch hatte Spaß.\\n',\n",
              " '\\tIch habe mich amüsiert.\\n',\n",
              " '\\tIch habe Tom geschlagen.\\n',\n",
              " '\\tDas hoffe ich.\\n',\n",
              " '\\tIch hoffe es.\\n',\n",
              " '\\tIch habe aufgelegt.\\n',\n",
              " '\\tIch wusste es.\\n',\n",
              " '\\tIch wusste das.\\n',\n",
              " '\\tIch lachte.\\n',\n",
              " '\\tDas gefällt mir.\\n',\n",
              " '\\tIch habe ihn verloren.\\n',\n",
              " '\\tIch liebe es!\\n',\n",
              " '\\tIch liebe es.\\n',\n",
              " '\\tIch habe es gemacht.\\n',\n",
              " '\\tIch habe es geschafft.\\n',\n",
              " '\\tVielleicht gewinne ich.\\n',\n",
              " '\\tIch meine es so!\\n',\n",
              " '\\tIch meine es ernst!\\n',\n",
              " '\\tIch meine es ernst.\\n',\n",
              " '\\tEs ist ernst gemeint von mir.\\n',\n",
              " '\\tIch habe Tom getroffen.\\n',\n",
              " '\\tIch traf Tom.\\n',\n",
              " '\\tIch bin Tom begegnet.\\n',\n",
              " '\\tIch habe ihn getroffen.\\n',\n",
              " '\\tIch vermisse es.\\n',\n",
              " '\\tEs fehlt mir.\\n',\n",
              " \"\\tIch versprech's.\\n\",\n",
              " '\\tIch verspreche es.\\n',\n",
              " '\\tIch sagte nein.\\n',\n",
              " '\\tIch habe Tom gesehen.\\n',\n",
              " '\\tIch habe ihn gesehen.\\n',\n",
              " '\\tIch habe einen gesehen.\\n',\n",
              " '\\tIch habe dich gesehen.\\n',\n",
              " '\\tIch habe euch gesehen.\\n',\n",
              " '\\tIch sehe Tom.\\n',\n",
              " '\\tIch nahm es.\\n',\n",
              " '\\tIch habe es genommen.\\n',\n",
              " '\\tIch möchte es.\\n',\n",
              " '\\tIch will es.\\n',\n",
              " '\\tIch war neu.\\n',\n",
              " '\\tIch war schüchtern.\\n',\n",
              " '\\tIch gehe.\\n',\n",
              " '\\tIch werde gehen.\\n',\n",
              " '\\tIch wachte auf.\\n',\n",
              " '\\tIch würde es tun.\\n',\n",
              " '\\tIch würde gehen.\\n',\n",
              " '\\tIch werde anrufen.\\n',\n",
              " '\\tIch werde kommen.\\n',\n",
              " '\\tIch werde kochen.\\n',\n",
              " '\\tIch helfe.\\n',\n",
              " '\\tIch werde leben.\\n',\n",
              " '\\tIch werde gehorchen.\\n',\n",
              " '\\tIch werde aufhören.\\n',\n",
              " '\\tIch werde abbrechen.\\n',\n",
              " '\\tIch werde singen.\\n',\n",
              " '\\tIch werde bleiben.\\n',\n",
              " '\\tIch werde aufhören.\\n',\n",
              " '\\tIch werde reden.\\n',\n",
              " '\\tIch werde warten.\\n',\n",
              " '\\tIch gehe zu Fuß.\\n',\n",
              " '\\tIch bin ein Mann.\\n',\n",
              " '\\tIch bin sauer.\\n',\n",
              " '\\tIch bin wach.\\n',\n",
              " '\\tIch bin blind.\\n',\n",
              " '\\tIch bin gelangweilt.\\n',\n",
              " '\\tMir ist langweilig.\\n',\n",
              " '\\tIch bin knapp bei Kasse.\\n',\n",
              " '\\tIch bin pleite.\\n',\n",
              " '\\tIch bin geheilt.\\n',\n",
              " '\\tIch bin betrunken.\\n',\n",
              " '\\tIch bin blau.\\n',\n",
              " '\\tIch werde bald sterben.\\n',\n",
              " '\\tIch bin früh dran.\\n',\n",
              " '\\tIch bin Erster.\\n',\n",
              " '\\tIch bin Erste.\\n',\n",
              " '\\tIch gehe jetzt.\\n',\n",
              " '\\tIch bin glücklich.\\n',\n",
              " '\\tIch bin froh.\\n',\n",
              " '\\tIch lüge.\\n',\n",
              " '\\tIch bin bedürftig.\\n',\n",
              " '\\tIch bin fett.\\n',\n",
              " '\\tIch bin pingelig.\\n',\n",
              " '\\tIch bin wählerisch.\\n',\n",
              " '\\tIch bin soweit!\\n',\n",
              " '\\tIch bin soweit.\\n',\n",
              " '\\tIch bin so weit.\\n',\n",
              " '\\tIch habe recht.\\n',\n",
              " '\\tIch habe Recht.\\n',\n",
              " '\\tIch bin nüchtern.\\n',\n",
              " '\\tEs tut mir leid.\\n',\n",
              " '\\tEntschuldigung!\\n',\n",
              " '\\tIch stecke fest.\\n',\n",
              " '\\tIch bin müde!\\n',\n",
              " '\\tIch bin müde.\\n',\n",
              " '\\tIch bin zäh.\\n',\n",
              " '\\tIch bin verärgert.\\n',\n",
              " '\\tIch bin bestürzt.\\n',\n",
              " '\\tIch gehöre dir.\\n',\n",
              " '\\tIch habe verloren.\\n',\n",
              " '\\tBeachte es gar nicht!\\n',\n",
              " '\\tBeachten Sie es gar nicht!\\n',\n",
              " '\\tIst Tom okay?\\n',\n",
              " '\\tIst Tom da?\\n',\n",
              " '\\tIst er Tom?\\n',\n",
              " '\\tIst es schlimm?\\n',\n",
              " '\\tIst das weit?\\n',\n",
              " '\\tIst es heiß?\\n',\n",
              " '\\tIst es neu?\\n',\n",
              " '\\tEs brannte.\\n',\n",
              " '\\tEs verbrannte.\\n',\n",
              " '\\tEs hat geschneit.\\n',\n",
              " '\\tEs hat geklappt.\\n',\n",
              " '\\tDas hat funktioniert.\\n',\n",
              " '\\tEs ist halb vier.\\n',\n",
              " '\\tEs ist 7:45 Uhr.\\n',\n",
              " '\\tEs ist sieben Uhr fünfundvierzig.\\n',\n",
              " '\\tEs ist 8:30 Uhr.\\n',\n",
              " '\\tEs ist Viertel nach neun.\\n',\n",
              " '\\tEs ist 9.15\\xa0Uhr.\\n',\n",
              " '\\tDas ist ein Fernseher.\\n',\n",
              " '\\tEs ist kalt.\\n',\n",
              " '\\tEs ist kühl.\\n',\n",
              " '\\tEs ist dunkel.\\n',\n",
              " '\\tDas ist einfach.\\n',\n",
              " '\\tEs ist einfach.\\n',\n",
              " '\\tEs ist gerecht.\\n',\n",
              " '\\tEs ist frei.\\n',\n",
              " '\\tDas ist hart.\\n',\n",
              " '\\tEs ist hier.\\n',\n",
              " '\\tEs ist spät.\\n',\n",
              " '\\tEs ist meins.\\n',\n",
              " '\\tSie gehört mir.\\n',\n",
              " '\\tEr gehört mir.\\n',\n",
              " '\\tEs ist in Ordnung.\\n',\n",
              " '\\tEs ist geöffnet.\\n',\n",
              " '\\tEs ist unsers.\\n',\n",
              " '\\tEs ist vorbei.\\n',\n",
              " '\\tEs ist Sand.\\n',\n",
              " '\\tEs ist wahr.\\n',\n",
              " '\\tEs ist Arbeit.\\n',\n",
              " '\\tDas ist meine Arbeit.\\n',\n",
              " '\\tSpring runter!\\n',\n",
              " '\\tBleib weg.\\n',\n",
              " '\\tBleibt weg.\\n',\n",
              " '\\tBleiben Sie weg.\\n',\n",
              " '\\tBleiben Sie zurück.\\n',\n",
              " '\\tBleib gelassen!\\n',\n",
              " '\\tBehalt’s!\\n',\n",
              " '\\tBehalte sie.\\n',\n",
              " '\\tBehalten Sie sie.\\n',\n",
              " '\\tBehaltet sie.\\n',\n",
              " '\\tBehalt sie!\\n',\n",
              " '\\tHalt dich warm.\\n',\n",
              " '\\tHaltet euch warm.\\n',\n",
              " '\\tHalten Sie sich warm.\\n',\n",
              " '\\tVerlasse Tom.\\n',\n",
              " '\\tVerlassen Sie Tom.\\n',\n",
              " '\\tGeh jetzt!\\n',\n",
              " '\\tGeht jetzt!\\n',\n",
              " '\\tLass es sein.\\n',\n",
              " '\\tLass es bleiben.\\n',\n",
              " '\\tLass mich in Ruhe.\\n',\n",
              " '\\tLassen Sie mich gehen!\\n',\n",
              " '\\tLass mich gehen!\\n',\n",
              " '\\tLass mich herein.\\n',\n",
              " '\\tLass uns gehen.\\n',\n",
              " '\\tLassen Sie uns gehen.\\n',\n",
              " '\\tEssen wir etwas!\\n',\n",
              " '\\tSchauen wir mal.\\n',\n",
              " '\\tLasst es uns versuchen!\\n',\n",
              " '\\tLass es uns versuchen!\\n',\n",
              " '\\tLass es uns probieren!\\n',\n",
              " '\\tLieg still und beweg dich nicht.\\n',\n",
              " '\\tHör zu.\\n',\n",
              " '\\tSchau nach hinten!\\n',\n",
              " '\\tGuck runter.\\n',\n",
              " '\\tSchau hier.\\n',\n",
              " '\\tSchau her.\\n',\n",
              " '\\tDreh es auf.\\n',\n",
              " '\\tMach dich locker!\\n',\n",
              " '\\tRutsch mal ein Stück!\\n',\n",
              " '\\tGuter Schuss!\\n',\n",
              " '\\tNatürlich!\\n',\n",
              " '\\tSelbstverständlich!\\n',\n",
              " '\\tAuf jeden Fall!\\n',\n",
              " '\\tNa klar!\\n',\n",
              " '\\tAber sicher doch!\\n',\n",
              " '\\tBitte geh.\\n',\n",
              " '\\tBitte geht.\\n',\n",
              " '\\tBitte gehen Sie.\\n',\n",
              " '\\tSetz es auf.\\n',\n",
              " '\\tTu es drauf.\\n',\n",
              " '\\tSetz ihn auf.\\n',\n",
              " '\\tZieh sie an.\\n',\n",
              " '\\tSetz sie auf.\\n',\n",
              " '\\tSetz sie dir auf.\\n',\n",
              " '\\tZieh sie dir an.\\n',\n",
              " '\\tSetz ihn dir auf.\\n',\n",
              " '\\tLies das hier.\\n',\n",
              " '\\tSag „Ah!“\\n',\n",
              " '\\tSag Hallo.\\n',\n",
              " '\\tSiehe unten.\\n',\n",
              " '\\tFass ihn!\\n',\n",
              " '\\tFassen Sie ihn!\\n',\n",
              " '\\tFasst ihn!\\n',\n",
              " '\\tWirklich?\\n',\n",
              " '\\tEcht?\\n',\n",
              " '\\tErnsthaft?\\n',\n",
              " '\\tIm Ernst?\\n',\n",
              " '\\tSie weinte.\\n',\n",
              " \"\\tSie hat's versucht.\\n\",\n",
              " '\\tSie geht.\\n',\n",
              " '\\tSie geht zu Fuß.\\n',\n",
              " '\\tUnterschreibe hier.\\n',\n",
              " '\\tUnterschreiben Sie hier.\\n',\n",
              " '\\tUnterschreiben Sie das.\\n',\n",
              " '\\tUnterschreib das.\\n',\n",
              " '\\tSetz dich zu mir!\\n',\n",
              " '\\tSitz still.\\n',\n",
              " '\\tSitzen Sie still.\\n',\n",
              " '\\tSetz dich dorthin.\\n',\n",
              " '\\tHarre aus!\\n',\n",
              " '\\tBeginnen Sie jetzt.\\n',\n",
              " '\\tFang jetzt an.\\n',\n",
              " '\\tBleiben Sie weg.\\n',\n",
              " '\\tBleibt zurück!\\n',\n",
              " '\\tBleib ruhig.\\n',\n",
              " '\\tBleiben Sie ruhig.\\n',\n",
              " '\\tBleib ruhig.\\n',\n",
              " '\\tBleibt ruhig.\\n',\n",
              " '\\tBleiben Sie ruhig.\\n',\n",
              " '\\tBleib cool.\\n',\n",
              " '\\tBleibt cool.\\n',\n",
              " '\\tBleib unten!\\n',\n",
              " '\\tBleib unten!\\n',\n",
              " '\\tBleiben Sie dünn.\\n',\n",
              " '\\tBleib dünn.\\n',\n",
              " '\\tTritt zurück!\\n',\n",
              " '\\tHör hier auf.\\n',\n",
              " '\\tHalte hier.\\n',\n",
              " '\\tHalte hier an.\\n',\n",
              " '\\tBleib hier stehen.\\n',\n",
              " '\\tLass das!\\n',\n",
              " '\\tHör auf damit!\\n',\n",
              " '\\tHör mal auf!\\n',\n",
              " '\\tHör mal auf damit!\\n',\n",
              " '\\tStopp sie.\\n',\n",
              " '\\tStoppen Sie sie.\\n',\n",
              " '\\tPass auf dich auf!\\n',\n",
              " \"\\tMach's gut.\\n\",\n",
              " '\\tNimm meins.\\n',\n",
              " '\\tNimm meine.\\n',\n",
              " '\\tNimm meinen.\\n',\n",
              " '\\tNehmt meins.\\n',\n",
              " '\\tNehmt meine.\\n',\n",
              " '\\tNehmt meinen.\\n',\n",
              " '\\tNehmen Sie meins.\\n',\n",
              " '\\tNehmen Sie meinen.\\n',\n",
              " '\\tNehmen Sie meine.\\n',\n",
              " '\\tÜbernimm du.\\n',\n",
              " '\\tÜbernehmt ihr.\\n',\n",
              " '\\tÜbernehmen Sie.\\n',\n",
              " '\\tNimm das.\\n',\n",
              " '\\tDanke!\\n',\n",
              " '\\tDas ist in Ordnung.\\n',\n",
              " '\\tDas ist o.k.\\n',\n",
              " \"\\tDas war's.\\n\",\n",
              " '\\tDas bin ich!\\n',\n",
              " '\\tIch bin jetzt fertig.\\n',\n",
              " '\\tNa und?\\n',\n",
              " '\\tSie fielen.\\n',\n",
              " '\\tSie stürzten.\\n',\n",
              " '\\tSie haben gelogen.\\n',\n",
              " '\\tSie verloren.\\n',\n",
              " '\\tSie haben verloren.\\n',\n",
              " '\\tSie schwammen.\\n',\n",
              " '\\tDie Zeit ist um.\\n',\n",
              " '\\tTom verneigte sich.\\n',\n",
              " '\\tTom weinte.\\n',\n",
              " '\\tTom döste.\\n',\n",
              " '\\tTom fuhr.\\n',\n",
              " '\\tTom erstarrte.\\n',\n",
              " '\\tTom ist in Ordnung.\\n',\n",
              " '\\tTom ist hier.\\n',\n",
              " '\\tTom ist auf.\\n',\n",
              " '\\tTom strickt.\\n',\n",
              " '\\tTom weiß es.\\n',\n",
              " '\\tTom bewegte sich.\\n',\n",
              " '\\tTom schlief.\\n',\n",
              " '\\tTom hat gesprochen.\\n',\n",
              " '\\tTom sprach.\\n',\n",
              " '\\tTom schwimmt.\\n',\n",
              " '\\tTom versuchte es.\\n',\n",
              " '\\tTom hat es versucht.\\n',\n",
              " '\\tTom versucht es.\\n',\n",
              " '\\tTom hat gewählt.\\n',\n",
              " '\\tTom wählte.\\n',\n",
              " '\\tTom geht zu Fuß.\\n',\n",
              " '\\tTom winkte.\\n',\n",
              " '\\tTom arbeitet.\\n',\n",
              " '\\tTom wird gehen.\\n',\n",
              " '\\tTom ist dick.\\n',\n",
              " '\\tTom ist wütend.\\n',\n",
              " '\\tTom ist traurig.\\n',\n",
              " '\\tTom ist schüchtern.\\n',\n",
              " '\\tVertraue Tom!\\n',\n",
              " '\\tVertrauen Sie Tom!\\n',\n",
              " '\\tVertraut Tom!\\n',\n",
              " '\\tVersuchen Sie es noch einmal.\\n',\n",
              " '\\tVersuch es noch einmal.\\n',\n",
              " '\\tVersucht es noch mal.\\n',\n",
              " '\\tProbier es an!\\n',\n",
              " '\\tProbier sie an!\\n',\n",
              " '\\tProbier ihn an!\\n',\n",
              " '\\tSchließ es auf.\\n',\n",
              " '\\tEntsperr es.\\n',\n",
              " '\\tSieh Tom zu!\\n',\n",
              " '\\tSehen Sie Tom zu!\\n',\n",
              " '\\tSeht Tom zu!\\n',\n",
              " '\\tWir waren uns einig.\\n',\n",
              " '\\tWir stimmten zu.\\n',\n",
              " '\\tWir können gehen.\\n',\n",
              " '\\tWir sahen es.\\n',\n",
              " '\\tWir haben es gesehen.\\n',\n",
              " '\\tWir haben uns unterhalten.\\n',\n",
              " '\\tWir warteten.\\n',\n",
              " '\\tWir sind zu Fuß gegangen.\\n',\n",
              " '\\tWir werden sehen.\\n',\n",
              " '\\tWir werden es versuchen.\\n',\n",
              " '\\tWir werden siegen.\\n',\n",
              " '\\tUns ist heiß.\\n',\n",
              " '\\tWir sind traurig.\\n',\n",
              " '\\tWir sind schüchtern.\\n',\n",
              " '\\tWir haben gewonnen!\\n',\n",
              " '\\tWas ist los?\\n',\n",
              " '\\tWas ist los?\\n',\n",
              " \"\\tNa, wie geht's?\\n\",\n",
              " '\\tWen kümmert’s?\\n',\n",
              " '\\tWen kümmert das schon?\\n',\n",
              " '\\tWer ist er?\\n',\n",
              " '\\tWer hat gesprochen?\\n',\n",
              " '\\tWer sprach?\\n',\n",
              " '\\tWer stand?\\n',\n",
              " '\\tWer geht?\\n',\n",
              " '\\tWer wird gehen?\\n',\n",
              " '\\tWer ist sie?\\n',\n",
              " '\\tWunderbar!\\n',\n",
              " '\\tHerrlich!\\n',\n",
              " '\\tSchreibe Tom!\\n',\n",
              " '\\tSchreiben Sie Tom!\\n',\n",
              " '\\tSchreibt Tom!\\n',\n",
              " '\\tDu fährst.\\n',\n",
              " '\\tSie fahren.\\n',\n",
              " '\\tDu Idiot!\\n',\n",
              " '\\tDu fängst an.\\n',\n",
              " '\\tDu stinkst.\\n',\n",
              " '\\tIhr stinkt.\\n',\n",
              " '\\tSie stinken.\\n',\n",
              " \"\\tDu hast's versucht.\\n\",\n",
              " '\\tDu bist okay.\\n',\n",
              " '\\tZiel höher!\\n',\n",
              " '\\tZielen Sie höher!\\n',\n",
              " '\\tZielt höher!\\n',\n",
              " '\\tAlle Mann an Bord!\\n',\n",
              " '\\tSterbe ich?\\n',\n",
              " '\\tBin ich früh dran?\\n',\n",
              " '\\tBin ich gefeuert?\\n',\n",
              " '\\tBin ich eingestellt?\\n',\n",
              " '\\tHabe ich recht?\\n',\n",
              " '\\tLiege ich richtig?\\n',\n",
              " '\\tBin ich komisch?\\n',\n",
              " '\\tHab ich nicht Recht?\\n',\n",
              " '\\tIrre ich mich?\\n',\n",
              " '\\tAntworte Tom!\\n',\n",
              " '\\tAntworten Sie Tom!\\n',\n",
              " '\\tAntwortet Tom!\\n',\n",
              " '\\tBist du 18?\\n',\n",
              " '\\tGeht es dir gut?\\n',\n",
              " '\\tGeht es euch gut?\\n',\n",
              " '\\tGeht es Ihnen gut?\\n',\n",
              " '\\tBist du dabei?\\n',\n",
              " '\\tBist du auf?\\n',\n",
              " '\\tFrag irgendwen!\\n',\n",
              " '\\tFrag herum.\\n',\n",
              " '\\tFragen Sie herum.\\n',\n",
              " '\\tFragt herum.\\n',\n",
              " '\\tSei vorsichtig!\\n',\n",
              " '\\tSieh dich vor!\\n',\n",
              " '\\tGib Obacht!\\n',\n",
              " '\\tGib Acht!\\n',\n",
              " '\\tGib acht!\\n',\n",
              " '\\tSei geduldig!\\n',\n",
              " '\\tBleib mal auf dem Boden der Tatsachen!\\n',\n",
              " '\\tVögel singen.\\n',\n",
              " '\\tBring Wein.\\n',\n",
              " '\\tKann ich kommen?\\n',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRI3RbxAEWVb",
        "colab_type": "text"
      },
      "source": [
        "#**English to Finish Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjCTwqugEpQn",
        "colab_type": "code",
        "outputId": "b0914929-7575-466f-a72a-90f784c4f621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('fin.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and finish sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each finish character - key is index and value is finish character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get finish character given its index - key is finish character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and finish sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.7716 - val_loss: 0.8338\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.6343 - val_loss: 0.7009\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.5586 - val_loss: 0.6523\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.5195 - val_loss: 0.6291\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4880 - val_loss: 0.5782\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4617 - val_loss: 0.5605\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4396 - val_loss: 0.5479\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4211 - val_loss: 0.5309\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4047 - val_loss: 0.5152\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3901 - val_loss: 0.5058\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3768 - val_loss: 0.4949\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3642 - val_loss: 0.4851\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.3522 - val_loss: 0.4811\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3411 - val_loss: 0.4794\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3307 - val_loss: 0.4672\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3201 - val_loss: 0.4661\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3104 - val_loss: 0.4624\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3010 - val_loss: 0.4607\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2922 - val_loss: 0.4592\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2839 - val_loss: 0.4609\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2756 - val_loss: 0.4581\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2676 - val_loss: 0.4591\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2602 - val_loss: 0.4573\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2528 - val_loss: 0.4586\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2456 - val_loss: 0.4614\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2390 - val_loss: 0.4633\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2326 - val_loss: 0.4643\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2259 - val_loss: 0.4642\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2198 - val_loss: 0.4719\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2141 - val_loss: 0.4721\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2083 - val_loss: 0.4730\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2027 - val_loss: 0.4808\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1976 - val_loss: 0.4832\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1924 - val_loss: 0.4855\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1879 - val_loss: 0.4910\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1828 - val_loss: 0.4949\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1785 - val_loss: 0.4996\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1739 - val_loss: 0.5045\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1695 - val_loss: 0.5100\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1659 - val_loss: 0.5109\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1613 - val_loss: 0.5166\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1580 - val_loss: 0.5205\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1540 - val_loss: 0.5267\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1508 - val_loss: 0.5309\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1471 - val_loss: 0.5346\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1438 - val_loss: 0.5442\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1407 - val_loss: 0.5486\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1376 - val_loss: 0.5518\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1343 - val_loss: 0.5561\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1321 - val_loss: 0.5598\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Mene.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Hervo!\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Hervo!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Anta kerta!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Anta kerta!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Pussuudekse.\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Kuka?\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Vau!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Vau!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Vau!\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Tulta!\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Auttakaa!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rodang9rE_fV",
        "colab_type": "text"
      },
      "source": [
        "#**English to Hungarian Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08iEirPNFJBO",
        "colab_type": "code",
        "outputId": "e2025b6f-c2b5-45dd-c0c8-bbea1a026df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('hun.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and hungarian sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each hungarian character - key is index and value is hungarian character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get hungarian character given its index - key is hungarian character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and hungarian sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.8631 - val_loss: 0.9275\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.7229 - val_loss: 0.7714\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.6300 - val_loss: 0.7110\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.5812 - val_loss: 0.6694\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.5464 - val_loss: 0.6457\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.5191 - val_loss: 0.6197\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4958 - val_loss: 0.5973\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4746 - val_loss: 0.5799\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4557 - val_loss: 0.5685\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4385 - val_loss: 0.5512\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4225 - val_loss: 0.5423\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4077 - val_loss: 0.5353\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3944 - val_loss: 0.5237\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3809 - val_loss: 0.5190\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3690 - val_loss: 0.5141\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3571 - val_loss: 0.5066\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3463 - val_loss: 0.5040\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3355 - val_loss: 0.5026\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3252 - val_loss: 0.5001\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3151 - val_loss: 0.4976\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3056 - val_loss: 0.4989\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2963 - val_loss: 0.4960\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2874 - val_loss: 0.4985\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2786 - val_loss: 0.4988\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2702 - val_loss: 0.4975\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2617 - val_loss: 0.5011\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2540 - val_loss: 0.5032\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2463 - val_loss: 0.5059\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2384 - val_loss: 0.5121\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2314 - val_loss: 0.5111\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2241 - val_loss: 0.5175\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2174 - val_loss: 0.5226\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2109 - val_loss: 0.5287\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2048 - val_loss: 0.5311\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1988 - val_loss: 0.5371\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1928 - val_loss: 0.5423\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1870 - val_loss: 0.5493\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1816 - val_loss: 0.5506\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1759 - val_loss: 0.5596\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1706 - val_loss: 0.5626\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1661 - val_loss: 0.5701\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1612 - val_loss: 0.5787\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1562 - val_loss: 0.5813\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1515 - val_loss: 0.5896\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1473 - val_loss: 0.5996\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1432 - val_loss: 0.6002\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1397 - val_loss: 0.6088\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1355 - val_loss: 0.6088\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1320 - val_loss: 0.6233\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1281 - val_loss: 0.6278\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Cső!\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Cső!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Rohanj!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Rohanj!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Rohanj!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Rohanj!\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Ki?\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Hűha!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Hűha!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Hűha!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Hűha!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Hűha!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYS7uvIBFeDQ",
        "colab_type": "text"
      },
      "source": [
        "#**English to Italian Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNIApTThFwEb",
        "colab_type": "code",
        "outputId": "68f0b7a9-056f-48ec-91d4-5476a0f221f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('ita.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and italian sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each italian character - key is index and value is italian character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get italian character given its index - key is italian character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and italian sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 1.0061 - val_loss: 1.0371\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.8244 - val_loss: 0.8373\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.7107 - val_loss: 0.7492\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.6445 - val_loss: 0.7134\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.5977 - val_loss: 0.6806\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.5589 - val_loss: 0.6515\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.5258 - val_loss: 0.6313\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.4963 - val_loss: 0.6110\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.4680 - val_loss: 0.5933\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.4428 - val_loss: 0.5789\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4185 - val_loss: 0.5779\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3963 - val_loss: 0.5636\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3757 - val_loss: 0.5606\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3562 - val_loss: 0.5588\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3387 - val_loss: 0.5560\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3211 - val_loss: 0.5560\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3057 - val_loss: 0.5586\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2909 - val_loss: 0.5528\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2774 - val_loss: 0.5558\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2644 - val_loss: 0.5547\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2529 - val_loss: 0.5546\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2413 - val_loss: 0.5611\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2314 - val_loss: 0.5625\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2219 - val_loss: 0.5716\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2125 - val_loss: 0.5750\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2047 - val_loss: 0.5748\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1967 - val_loss: 0.5773\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1892 - val_loss: 0.5918\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1823 - val_loss: 0.5950\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1759 - val_loss: 0.6048\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1698 - val_loss: 0.6146\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1645 - val_loss: 0.6149\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1587 - val_loss: 0.6221\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1536 - val_loss: 0.6243\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1486 - val_loss: 0.6386\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1438 - val_loss: 0.6446\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1399 - val_loss: 0.6567\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1360 - val_loss: 0.6596\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1321 - val_loss: 0.6641\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1281 - val_loss: 0.6658\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1241 - val_loss: 0.6822\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1211 - val_loss: 0.6771\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1178 - val_loss: 0.6916\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1145 - val_loss: 0.6891\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1122 - val_loss: 0.6943\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1088 - val_loss: 0.6991\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1060 - val_loss: 0.7133\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1033 - val_loss: 0.7101\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1014 - val_loss: 0.7126\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0986 - val_loss: 0.7398\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Ciao!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Corri!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Corri!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Corri!\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Chi?\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Moragli!\n",
            "\n",
            "-\n",
            "Input sentence: Jump!\n",
            "Decoded sentence: Salta!\n",
            "\n",
            "-\n",
            "Input sentence: Jump!\n",
            "Decoded sentence: Salta!\n",
            "\n",
            "-\n",
            "Input sentence: Jump!\n",
            "Decoded sentence: Salta!\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Saltate.\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Saltate.\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Saltate.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKJLvQjlFxia",
        "colab_type": "text"
      },
      "source": [
        "#**English to Dutch Taranslation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSq9C3N7HhzW",
        "colab_type": "code",
        "outputId": "97ab5821-5f79-4485-9785-9f101339e3ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('nld.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and dutch sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each dutch character - key is index and value is dutch character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get dutch character given its index - key is dutch character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and dutch sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.9656 - val_loss: 1.0296\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.7619 - val_loss: 0.8775\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.6612 - val_loss: 0.7824\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.6071 - val_loss: 0.7531\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.5653 - val_loss: 0.7062\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.5341 - val_loss: 0.6717\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.5071 - val_loss: 0.6593\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4836 - val_loss: 0.6325\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4636 - val_loss: 0.6143\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4452 - val_loss: 0.6006\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4292 - val_loss: 0.5892\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4147 - val_loss: 0.5737\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4016 - val_loss: 0.5684\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3892 - val_loss: 0.5572\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3770 - val_loss: 0.5545\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3664 - val_loss: 0.5493\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3556 - val_loss: 0.5417\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3456 - val_loss: 0.5412\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3363 - val_loss: 0.5347\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3275 - val_loss: 0.5346\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3182 - val_loss: 0.5313\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3094 - val_loss: 0.5277\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3015 - val_loss: 0.5332\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2935 - val_loss: 0.5298\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2855 - val_loss: 0.5285\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2780 - val_loss: 0.5319\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2705 - val_loss: 0.5328\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2635 - val_loss: 0.5391\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2564 - val_loss: 0.5378\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2499 - val_loss: 0.5378\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2430 - val_loss: 0.5386\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2368 - val_loss: 0.5440\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2309 - val_loss: 0.5427\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2248 - val_loss: 0.5490\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2189 - val_loss: 0.5534\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2133 - val_loss: 0.5601\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2078 - val_loss: 0.5674\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2028 - val_loss: 0.5646\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1978 - val_loss: 0.5743\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1928 - val_loss: 0.5719\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1878 - val_loss: 0.5823\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1832 - val_loss: 0.5821\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1786 - val_loss: 0.5977\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1742 - val_loss: 0.5985\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1699 - val_loss: 0.5945\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1659 - val_loss: 0.6077\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1622 - val_loss: 0.6127\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1582 - val_loss: 0.6204\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1544 - val_loss: 0.6222\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1508 - val_loss: 0.6300\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Hoi.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Hoi.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Hoi.\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Briljant!\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Wie?\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Wacht.\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Brand!\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Brand!\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Hoi!\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: De praten.\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Ga tenoof ommaat.\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Ga tenoof ommaat.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJfwp27RHny2",
        "colab_type": "text"
      },
      "source": [
        "#**English to Portuguese Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ifs11AkIBX6",
        "colab_type": "code",
        "outputId": "2a2a746a-8001-48fa-beb6-6421cf5c9496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('por.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and portuguese sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each portuguese character - key is index and value is portuguese character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get portuguese character given its index - key is portuguese character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and portuguese sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.0522 - val_loss: 1.0879\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.8400 - val_loss: 0.8844\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.7177 - val_loss: 0.7985\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.6587 - val_loss: 0.7504\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.6158 - val_loss: 0.6917\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.5755 - val_loss: 0.6647\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5426 - val_loss: 0.6310\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.5128 - val_loss: 0.6142\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4875 - val_loss: 0.5933\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.4654 - val_loss: 0.5731\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4447 - val_loss: 0.5598\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.4254 - val_loss: 0.5454\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.4083 - val_loss: 0.5390\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3918 - val_loss: 0.5304\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3765 - val_loss: 0.5244\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3626 - val_loss: 0.5153\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3490 - val_loss: 0.5128\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3361 - val_loss: 0.5053\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3242 - val_loss: 0.5058\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3129 - val_loss: 0.5001\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3022 - val_loss: 0.5055\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2917 - val_loss: 0.5037\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2822 - val_loss: 0.4982\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2727 - val_loss: 0.5068\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2637 - val_loss: 0.5087\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2553 - val_loss: 0.5035\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2469 - val_loss: 0.5088\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2396 - val_loss: 0.5119\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2320 - val_loss: 0.5144\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2248 - val_loss: 0.5162\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2178 - val_loss: 0.5160\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2114 - val_loss: 0.5239\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2053 - val_loss: 0.5252\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1997 - val_loss: 0.5274\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1933 - val_loss: 0.5386\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1880 - val_loss: 0.5361\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1828 - val_loss: 0.5439\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1776 - val_loss: 0.5477\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1727 - val_loss: 0.5549\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1676 - val_loss: 0.5605\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1635 - val_loss: 0.5624\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1593 - val_loss: 0.5618\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1547 - val_loss: 0.5721\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1511 - val_loss: 0.5783\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1469 - val_loss: 0.5813\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1429 - val_loss: 0.5846\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1393 - val_loss: 0.5953\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1359 - val_loss: 0.5994\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1327 - val_loss: 0.6069\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1298 - val_loss: 0.6134\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Vá.\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Vá.\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Vá.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Oi.\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Corre!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Corre!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Corre!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Corre!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Corre!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Corre!\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Quem?\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Quem?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jEOYBfQINUH",
        "colab_type": "text"
      },
      "source": [
        "#**English to Russian Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXJL4ynKITnu",
        "colab_type": "code",
        "outputId": "48a0bf36-5a67-40aa-f924-ad7ee4340091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('rus.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and russian sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each russian character - key is index and value is russian character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get russian character given its index - key is russian character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and russian sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.7364 - val_loss: 0.7793\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.6004 - val_loss: 0.6679\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.5202 - val_loss: 0.6046\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4816 - val_loss: 0.5685\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4534 - val_loss: 0.5436\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4298 - val_loss: 0.5254\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4092 - val_loss: 0.5100\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3906 - val_loss: 0.5001\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3735 - val_loss: 0.4846\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3579 - val_loss: 0.4839\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3436 - val_loss: 0.4698\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3301 - val_loss: 0.4619\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3174 - val_loss: 0.4554\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3057 - val_loss: 0.4522\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2941 - val_loss: 0.4459\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2833 - val_loss: 0.4497\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2733 - val_loss: 0.4476\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2635 - val_loss: 0.4436\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2542 - val_loss: 0.4450\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2456 - val_loss: 0.4428\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2370 - val_loss: 0.4398\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2296 - val_loss: 0.4448\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2219 - val_loss: 0.4432\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2146 - val_loss: 0.4459\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2079 - val_loss: 0.4500\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2015 - val_loss: 0.4502\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1952 - val_loss: 0.4580\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1895 - val_loss: 0.4556\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1836 - val_loss: 0.4582\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1785 - val_loss: 0.4634\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1729 - val_loss: 0.4678\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1685 - val_loss: 0.4696\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1635 - val_loss: 0.4763\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1588 - val_loss: 0.4809\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1543 - val_loss: 0.4819\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1502 - val_loss: 0.4897\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1464 - val_loss: 0.4902\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1423 - val_loss: 0.4970\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1388 - val_loss: 0.5002\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1351 - val_loss: 0.5050\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1318 - val_loss: 0.5139\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1284 - val_loss: 0.5140\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1253 - val_loss: 0.5197\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1223 - val_loss: 0.5246\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1192 - val_loss: 0.5316\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1164 - val_loss: 0.5314\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1136 - val_loss: 0.5414\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1114 - val_loss: 0.5442\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1083 - val_loss: 0.5511\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1065 - val_loss: 0.5570\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Идите.\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Идите.\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Идите.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Бегите!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Бегите!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Бегите!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Бегите!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6pYCwjKIaOj",
        "colab_type": "text"
      },
      "source": [
        "#**English to Spanish Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92dV8HPKIuJR",
        "colab_type": "code",
        "outputId": "f6551534-a10a-47d2-e9dd-4d380f643179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('spa.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and spanish sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each spanish character - key is index and value is spanish character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get spanish character given its index - key is spanish character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and spanish sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 1.8916 - val_loss: 1.7018\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 1.3136 - val_loss: 1.2506\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 1.0592 - val_loss: 1.0870\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.9479 - val_loss: 1.0197\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.8873 - val_loss: 0.9654\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.8438 - val_loss: 0.9212\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.8090 - val_loss: 0.8976\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.7813 - val_loss: 0.8716\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.7578 - val_loss: 0.8521\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.7377 - val_loss: 0.8350\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.7196 - val_loss: 0.8263\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.7037 - val_loss: 0.8148\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.6899 - val_loss: 0.8118\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.6759 - val_loss: 0.7942\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.6629 - val_loss: 0.7849\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.6512 - val_loss: 0.7794\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.6402 - val_loss: 0.7717\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.6294 - val_loss: 0.7693\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.6193 - val_loss: 0.7619\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.6095 - val_loss: 0.7656\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.6001 - val_loss: 0.7579\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5910 - val_loss: 0.7578\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5822 - val_loss: 0.7565\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5733 - val_loss: 0.7547\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5653 - val_loss: 0.7468\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5570 - val_loss: 0.7460\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5491 - val_loss: 0.7454\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5414 - val_loss: 0.7502\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5340 - val_loss: 0.7513\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5267 - val_loss: 0.7453\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5191 - val_loss: 0.7536\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5122 - val_loss: 0.7463\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5052 - val_loss: 0.7500\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4985 - val_loss: 0.7495\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4920 - val_loss: 0.7523\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4854 - val_loss: 0.7581\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4788 - val_loss: 0.7614\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4731 - val_loss: 0.7629\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4669 - val_loss: 0.7584\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4605 - val_loss: 0.7644\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4545 - val_loss: 0.7705\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4489 - val_loss: 0.7738\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4432 - val_loss: 0.7740\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4373 - val_loss: 0.7799\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4318 - val_loss: 0.7810\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4264 - val_loss: 0.7856\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4211 - val_loss: 0.7895\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4153 - val_loss: 0.7919\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4111 - val_loss: 0.7987\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4051 - val_loss: 0.8032\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Vete. \n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Vete. \n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Vete. \n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Vete. \n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Hola. \n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: ¡Corra! \n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: ¡Corra! \n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: ¡Corra! \n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: ¡Corra! \n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Seltí. #2245555 (CK) & #4557717 (swyter)\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: ¿Quién está aquí? #2203844 (CK) & #6682523 (arh)\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: ¡Venoce disto! #2249824 (CK) & #4347537 (pchamorro)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrUoorj4JBTP",
        "colab_type": "text"
      },
      "source": [
        "#**English to Swedish Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Yd1pcnrJKB7",
        "colab_type": "code",
        "outputId": "f05d6062-efd5-431c-e52f-ea59cd3a9ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('swe.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and swedish sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each swedish character - key is index and value is swedish character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get swedish character given its index - key is swedish character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and swedish sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 1.0781 - val_loss: 1.1576\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.8406 - val_loss: 0.9500\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.7316 - val_loss: 0.8881\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.6783 - val_loss: 0.8361\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.6391 - val_loss: 0.7871\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.5997 - val_loss: 0.7583\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.5709 - val_loss: 0.7323\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.5462 - val_loss: 0.7187\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.5260 - val_loss: 0.6862\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.5070 - val_loss: 0.6710\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4903 - val_loss: 0.6637\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4755 - val_loss: 0.6468\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4610 - val_loss: 0.6469\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4491 - val_loss: 0.6296\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4361 - val_loss: 0.6413\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4243 - val_loss: 0.6240\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4127 - val_loss: 0.6191\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4018 - val_loss: 0.6140\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3913 - val_loss: 0.6110\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3814 - val_loss: 0.6162\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.3717 - val_loss: 0.6029\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.3618 - val_loss: 0.6070\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3531 - val_loss: 0.6066\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3438 - val_loss: 0.6035\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3351 - val_loss: 0.6043\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3269 - val_loss: 0.6075\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3188 - val_loss: 0.6115\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3112 - val_loss: 0.6105\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3033 - val_loss: 0.6158\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2956 - val_loss: 0.6197\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2887 - val_loss: 0.6242\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2810 - val_loss: 0.6223\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2745 - val_loss: 0.6285\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2678 - val_loss: 0.6371\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2611 - val_loss: 0.6450\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2548 - val_loss: 0.6466\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2486 - val_loss: 0.6490\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2426 - val_loss: 0.6554\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2371 - val_loss: 0.6602\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2314 - val_loss: 0.6664\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2264 - val_loss: 0.6681\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2206 - val_loss: 0.6782\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2163 - val_loss: 0.6882\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2110 - val_loss: 0.6934\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2061 - val_loss: 0.6967\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2017 - val_loss: 0.7052\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1973 - val_loss: 0.7152\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1931 - val_loss: 0.7172\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1885 - val_loss: 0.7207\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1844 - val_loss: 0.7388\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Skynda mig.\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Vem?\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Hjälp!\n",
            "\n",
            "-\n",
            "Input sentence: Jump!\n",
            "Decoded sentence: Slätt!\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Stung med dig.\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Skäfta in dig.\n",
            "\n",
            "-\n",
            "Input sentence: Wait.\n",
            "Decoded sentence: Skäfla Tom ett.\n",
            "\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Jag saknar.\n",
            "\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Jag saknar.\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Jag var orolig.\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Skämta mig det.\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Skäll!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EaVLtX7JSFE",
        "colab_type": "text"
      },
      "source": [
        "#**English to Turkish Translation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E7seQT6JSqt",
        "colab_type": "code",
        "outputId": "9d6b4c8a-a9ea-475a-98f8-dc74965eadac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('tur.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and turkish sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each turkish character - key is index and value is turkish character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get turkish character given its index - key is turkish character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and turkish sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 17s 2ms/step - loss: 1.1223 - val_loss: 1.1801\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.9016 - val_loss: 0.9693\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.7717 - val_loss: 0.8758\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.7102 - val_loss: 0.8146\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.6709 - val_loss: 0.7770\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.6394 - val_loss: 0.7486\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.6133 - val_loss: 0.7274\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.5903 - val_loss: 0.7057\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.5698 - val_loss: 0.6850\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.5500 - val_loss: 0.6688\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.5332 - val_loss: 0.6596\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.5172 - val_loss: 0.6495\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.5019 - val_loss: 0.6356\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4882 - val_loss: 0.6285\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4746 - val_loss: 0.6220\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4618 - val_loss: 0.6159\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4489 - val_loss: 0.6082\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4374 - val_loss: 0.6081\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4258 - val_loss: 0.5987\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4144 - val_loss: 0.5956\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4029 - val_loss: 0.5990\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3923 - val_loss: 0.5997\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.3814 - val_loss: 0.5958\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.3711 - val_loss: 0.5961\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3608 - val_loss: 0.6011\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3506 - val_loss: 0.6022\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3406 - val_loss: 0.5995\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3308 - val_loss: 0.6064\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3208 - val_loss: 0.6079\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3114 - val_loss: 0.6159\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.3019 - val_loss: 0.6212\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2932 - val_loss: 0.6264\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2840 - val_loss: 0.6357\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2751 - val_loss: 0.6364\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2663 - val_loss: 0.6488\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2584 - val_loss: 0.6571\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2502 - val_loss: 0.6568\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2425 - val_loss: 0.6684\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2342 - val_loss: 0.6797\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2271 - val_loss: 0.6886\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2196 - val_loss: 0.6914\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2121 - val_loss: 0.7051\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2058 - val_loss: 0.7140\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1991 - val_loss: 0.7239\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1927 - val_loss: 0.7335\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1864 - val_loss: 0.7430\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1804 - val_loss: 0.7558\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1741 - val_loss: 0.7702\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1688 - val_loss: 0.7723\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1633 - val_loss: 0.7858\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Merhaba.\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Kaç!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Kaç!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Kaç!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Kaç!\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Kim?\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Vay canına!\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Ateş!\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Ateş!\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Ateş!\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Amak!\n",
            "\n",
            "-\n",
            "Input sentence: Jump!\n",
            "Decoded sentence: Atla!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S7dd1XIJjzc",
        "colab_type": "text"
      },
      "source": [
        "#**English to Ukrainian Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDY86LnEKWrc",
        "colab_type": "code",
        "outputId": "3866e353-cb47-4089-f864-ac17df010f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('ukr.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and ukrainian sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each ukrainian character - key is index and value is ukrainian character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get ukrainian character given its index - key is ukrainian character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and ukrainian sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 1.0118 - val_loss: 1.0401\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.8205 - val_loss: 0.8343\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.7076 - val_loss: 0.7601\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.6542 - val_loss: 0.7234\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.6176 - val_loss: 0.6927\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5869 - val_loss: 0.6649\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5603 - val_loss: 0.6457\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5361 - val_loss: 0.6289\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.5131 - val_loss: 0.6187\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4926 - val_loss: 0.6039\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4734 - val_loss: 0.5890\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4553 - val_loss: 0.5790\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4383 - val_loss: 0.5756\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4222 - val_loss: 0.5738\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4066 - val_loss: 0.5653\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3918 - val_loss: 0.5559\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3782 - val_loss: 0.5541\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3648 - val_loss: 0.5513\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3514 - val_loss: 0.5483\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3400 - val_loss: 0.5517\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3288 - val_loss: 0.5538\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3173 - val_loss: 0.5518\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3075 - val_loss: 0.5483\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2969 - val_loss: 0.5514\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2872 - val_loss: 0.5549\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2784 - val_loss: 0.5607\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2694 - val_loss: 0.5645\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2618 - val_loss: 0.5650\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2538 - val_loss: 0.5656\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2459 - val_loss: 0.5743\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2387 - val_loss: 0.5745\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2320 - val_loss: 0.5801\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2250 - val_loss: 0.5810\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2190 - val_loss: 0.5866\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2127 - val_loss: 0.5908\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.2068 - val_loss: 0.5970\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2008 - val_loss: 0.5997\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1953 - val_loss: 0.6047\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1902 - val_loss: 0.6150\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1854 - val_loss: 0.6221\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1807 - val_loss: 0.6147\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1758 - val_loss: 0.6257\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1715 - val_loss: 0.6394\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1670 - val_loss: 0.6363\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1630 - val_loss: 0.6459\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1594 - val_loss: 0.6505\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1551 - val_loss: 0.6504\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1516 - val_loss: 0.6559\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1485 - val_loss: 0.6592\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1446 - val_loss: 0.6700\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Приносто!\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Привіт!\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Привіт!\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Привіт!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Біжіть!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Біжіть!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Біжіть!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Клас!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Клас!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Клас!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Клас!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Клас!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}